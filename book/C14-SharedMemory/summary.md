
-   -   [11. Storage and the Memory
        Hierarchy](../C11-MemHierarchy/index.html){.nav-link}
        -   [11.1. The Memory
            Hierarchy](../C11-MemHierarchy/mem_hierarchy.html){.nav-link}
        -   [11.2. Storage
            Devices](../C11-MemHierarchy/devices.html){.nav-link}
        -   [11.3.
            Locality](../C11-MemHierarchy/locality.html){.nav-link}
        -   [11.4. Caching](../C11-MemHierarchy/caching.html){.nav-link}
        -   [11.5. Cache Analysis and
            Cachegrind](../C11-MemHierarchy/cachegrind.html){.nav-link}
        -   [11.6. Looking Ahead: Caching on Multicore
            Processors](../C11-MemHierarchy/coherency.html){.nav-link}
        -   [11.7. Summary](../C11-MemHierarchy/summary.html){.nav-link}
        -   [11.8.
            Exercises](../C11-MemHierarchy/exercises.html){.nav-link}

-   -   [12. Code Optimization](../C12-CodeOpt/index.html){.nav-link}
        -   [12.1. First Steps](../C12-CodeOpt/basic.html){.nav-link}
        -   [12.2. Other Compiler
            Optimizations](../C12-CodeOpt/loops_functions.html){.nav-link}
        -   [12.3. Memory
            Considerations](../C12-CodeOpt/memory_considerations.html){.nav-link}
        -   [12.4. Summary](../C12-CodeOpt/summary.html){.nav-link}

-   -   [13. The Operating System](../C13-OS/index.html){.nav-link}
        -   [13.1. Booting and Running](../C13-OS/impl.html){.nav-link}
        -   [13.2. Processes](../C13-OS/processes.html){.nav-link}
        -   [13.3. Virtual Memory](../C13-OS/vm.html){.nav-link}
        -   [13.4. Interprocess
            Communication](../C13-OS/ipc.html){.nav-link}
            -   [13.4.1. Signals](../C13-OS/ipc_signals.html){.nav-link}
            -   [13.4.2. Message
                Passing](../C13-OS/ipc_msging.html){.nav-link}
            -   [13.4.3. Shared
                Memory](../C13-OS/ipc_shm.html){.nav-link}
        -   [13.5. Summary and Other OS
            Functionality](../C13-OS/advanced.html){.nav-link}
        -   [13.6. Exercises](../C13-OS/exercises.html){.nav-link}

-   -   [14. Leveraging Shared Memory in the Multicore
        Era](index.html){.nav-link}
        -   [14.1. Programming Multicore
            Systems](multicore.html){.nav-link}
        -   [14.2. POSIX Threads](posix.html){.nav-link}
        -   [14.3. Synchronizing
            Threads](synchronization.html){.nav-link}
            -   [14.3.1. Mutual Exclusion](mutex.html){.nav-link}
            -   [14.3.2. Semaphores](semaphores.html){.nav-link}
            -   [14.3.3. Other Synchronization
                Constructs](other_syncs.html){.nav-link}
        -   [14.4. Measuring Parallel
            Performance](performance.html){.nav-link}
            -   [14.4.1. Parallel Performance
                Basics](performance_basics.html){.nav-link}
            -   [14.4.2. Advanced
                Topics](performance_advanced.html){.nav-link}
        -   [14.5. Cache Coherence](cache_coherence.html){.nav-link}
        -   [14.6. Thread Safety](thread_safety.html){.nav-link}
        -   [14.7. Implicit Threading with
            OpenMP](openmp.html){.nav-link}
        -   [14.8. Summary](summary.html){.nav-link}
        -   [14.9. Exercises](exercises.html){.nav-link}

-   -   [15. Looking Ahead: Other Parallel
        Systems](../C15-Parallel/index.html){.nav-link}
        -   [15.1. Hardware Acceleration and
            CUDA](../C15-Parallel/gpu.html){.nav-link}
        -   [15.2. Distributed Memory
            Systems](../C15-Parallel/distrmem.html){.nav-link}
        -   [15.3. To Exascale and
            Beyond](../C15-Parallel/cloud.html){.nav-link}

-   -   [16. Appendix 1: Chapter 1 for Java
        Programmers](../Appendix1/index.html){.nav-link}
        -   [16.1. Getting Started Programming in
            C](../Appendix1/getting_started.html){.nav-link}
        -   [16.2. Input/Output (printf and
            scanf)](../Appendix1/input_output.html){.nav-link}
        -   [16.3. Conditionals and
            Loops](../Appendix1/conditionals.html){.nav-link}
        -   [16.4. Functions](../Appendix1/functions.html){.nav-link}
        -   [16.5. Arrays and
            Strings](../Appendix1/arrays_strings.html){.nav-link}
        -   [16.6. Structs](../Appendix1/structs.html){.nav-link}
        -   [16.7. Summary](../Appendix1/summary.html){.nav-link}
        -   [16.8. Exercises](../Appendix1/exercises.html){.nav-link}

-   -   [17. Appendix 2: Using Unix](../Appendix2/index.html){.nav-link}
        -   [17.1. Unix Command Line and the Unix File
            System](../Appendix2/cmdln_basics.html){.nav-link}
        -   [17.2. Man and the Unix
            Manual](../Appendix2/man.html){.nav-link}
        -   [17.3. Remote Access](../Appendix2/ssh_scp.html){.nav-link}
        -   [17.4. Unix Editors](../Appendix2/editors.html){.nav-link}
        -   [17.5. make and
            Makefiles](../Appendix2/makefiles.html){.nav-link}
        -   [17.6 Searching: grep and
            find](../Appendix2/grep.html){.nav-link}
        -   [17.7 File Permissions](../Appendix2/chmod.html){.nav-link}
        -   [17.8 Archiving and Compressing
            Files](../Appendix2/tar.html){.nav-link}
        -   [17.9 Process Control](../Appendix2/pskill.html){.nav-link}
        -   [17.10 Timing](../Appendix2/timing.html){.nav-link}
        -   [17.11 Command
            History](../Appendix2/history.html){.nav-link}
        -   [17.12 I/0
            Redirection](../Appendix2/ioredirect.html){.nav-link}
        -   [17.13 Pipes](../Appendix2/pipe.html){.nav-link}
        -   [17.14 Dot Files and
            .bashrc](../Appendix2/dotfiles.html){.nav-link}
        -   [17.15 Shell
            Programming](../Appendix2/shellprog.html){.nav-link}
        -   [17.16 Getting System
            Information](../Appendix2/sysinfo.html){.nav-link}



-   [Dive Into Systems](../index-2.html)
-   [14. Leveraging Shared Memory in the Multicore Era](index.html)
-   [14.8. Summary](summary.html)
:::

::: content
::: sect1
## [](#_summary){.anchor}14.8. Summary {#_summary}

::: sectionbody
::: paragraph
This chapter provided an overview of multicore processors and how to
program them. Specifically, we cover the POSIX threads (or Pthreads)
library and how to use it to create correct multithreaded programs that
speed up a single-threaded program's performance. Libraries like POSIX
and OpenMP utilize the **shared memory** model of communication, as
threads share data in a common memory space.
:::

### Key Takeaways {#_key_takeaways .discrete}

::: dlist

Threads are the fundamental unit of concurrent programs

:   To parallelize a serial program, programmers utilize lightweight
    constructs known as **threads**. For a particular multithreaded
    process, each thread has its own allocation of stack memory, but
    shares the program data, heap and instructions of the process. Like
    processes, threads run **nondeterministically** on the CPU (i.e.,
    the order of execution changes between runs, and which thread is
    assigned to which core is left up to the operating system).

Synchronization constructs ensure that programs work correctly

:   A consequence of shared memory is that threads can accidentally
    overwrite data residing in shared memory. A **race condition** can
    occur whenever two operations incorrectly update a shared value.
    When that shared value is data, a special type of race condition
    called a **data race** can arise. Synchronization constructs
    (mutexes, semaphores, etc.) help to guarantee program correctness by
    ensuring that threads execute one at a time when updating shared
    variables.

Be mindful when using synchronization constructs

:   Synchronization inherently introduces points of serial computation
    in an otherwise parallel program. It is therefore important to be
    aware of *how* one uses synchronization concepts. The set of
    operations that must run atomically is referred to as a **critical
    section**. If a critical section is too big, the threads will
    execute serially, yielding no improvement in runtime. Use
    synchronization constructs sloppily, and situations like
    **deadlock** may inadvertently arise. A good strategy is to have
    threads employ local variables as much as possible and update shared
    variables only when necessary.

Not all components of a program are parallelizable

:   Some programs necessarily have large serial components that can
    hinder a multithreaded program's performance on multiple cores
    (e.g., **Amdahl's Law**). Even when a high percentage of a program
    is parallelizable, speedup is rarely linear. Readers are also
    encouraged to look at other metrics such as efficiency and
    scalability when ascertaining the performance of their programs.
:::

### Further Reading {#_further_reading .discrete}

::: paragraph
This chapter is meant to give a taste of concurrency topics with
threads; it is by no means exhaustive. To learn more about programming
with POSIX threads and OpenMP, check out the excellent tutorials on
[Pthreads](https://hpc-tutorials.llnl.gov/posix/) and
[OpenMP](https://hpc.llnl.gov/tuts/openMP/) by Blaise Barney from
Lawrence Livermore National Labs. For automated tools for debugging
parallel programs, readers are encouraged to check out the
[Helgrind](https://valgrind.org/docs/manual/hg-manual.html) and
[DRD](https://valgrind.org/docs/manual/drd-manual.html) Valgrind tools.
:::

::: paragraph
In the [final
chapter](../C15-Parallel/index.html#_looking_ahead_other_parallel_systems_and_parallel_programming_models){.page}
of the book, we give a high-level overview of other common parallel
architectures and how to program them. [Read on to learn
more](../C15-Parallel/index.html#_looking_ahead_other_parallel_systems_and_parallel_programming_models){.page}.
:::
:::
:::

::: toc-menu
:::
:::
:::
:::

Copyright (C) 2020 Dive into Systems, LLC.

*Dive into Systems,* is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0
International](https://creativecommons.org/licenses/by-nc-nd/4.0/) (CC
BY-NC-ND 4.0).
