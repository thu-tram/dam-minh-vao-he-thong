
-   -   [11. Storage and the Memory
        Hierarchy](../C11-MemHierarchy/index.html){.nav-link}
        -   [11.1. The Memory
            Hierarchy](../C11-MemHierarchy/mem_hierarchy.html){.nav-link}
        -   [11.2. Storage
            Devices](../C11-MemHierarchy/devices.html){.nav-link}
        -   [11.3.
            Locality](../C11-MemHierarchy/locality.html){.nav-link}
        -   [11.4. Caching](../C11-MemHierarchy/caching.html){.nav-link}
        -   [11.5. Cache Analysis and
            Cachegrind](../C11-MemHierarchy/cachegrind.html){.nav-link}
        -   [11.6. Looking Ahead: Caching on Multicore
            Processors](../C11-MemHierarchy/coherency.html){.nav-link}
        -   [11.7. Summary](../C11-MemHierarchy/summary.html){.nav-link}
        -   [11.8.
            Exercises](../C11-MemHierarchy/exercises.html){.nav-link}

-   -   [12. Code Optimization](../C12-CodeOpt/index.html){.nav-link}
        -   [12.1. First Steps](../C12-CodeOpt/basic.html){.nav-link}
        -   [12.2. Other Compiler
            Optimizations](../C12-CodeOpt/loops_functions.html){.nav-link}
        -   [12.3. Memory
            Considerations](../C12-CodeOpt/memory_considerations.html){.nav-link}
        -   [12.4. Summary](../C12-CodeOpt/summary.html){.nav-link}

-   -   [13. The Operating System](../C13-OS/index.html){.nav-link}
        -   [13.1. Booting and Running](../C13-OS/impl.html){.nav-link}
        -   [13.2. Processes](../C13-OS/processes.html){.nav-link}
        -   [13.3. Virtual Memory](../C13-OS/vm.html){.nav-link}
        -   [13.4. Interprocess
            Communication](../C13-OS/ipc.html){.nav-link}
            -   [13.4.1. Signals](../C13-OS/ipc_signals.html){.nav-link}
            -   [13.4.2. Message
                Passing](../C13-OS/ipc_msging.html){.nav-link}
            -   [13.4.3. Shared
                Memory](../C13-OS/ipc_shm.html){.nav-link}
        -   [13.5. Summary and Other OS
            Functionality](../C13-OS/advanced.html){.nav-link}
        -   [13.6. Exercises](../C13-OS/exercises.html){.nav-link}

-   -   [14. Leveraging Shared Memory in the Multicore
        Era](index.html){.nav-link}
        -   [14.1. Programming Multicore
            Systems](multicore.html){.nav-link}
        -   [14.2. POSIX Threads](posix.html){.nav-link}
        -   [14.3. Synchronizing
            Threads](synchronization.html){.nav-link}
            -   [14.3.1. Mutual Exclusion](mutex.html){.nav-link}
            -   [14.3.2. Semaphores](semaphores.html){.nav-link}
            -   [14.3.3. Other Synchronization
                Constructs](other_syncs.html){.nav-link}
        -   [14.4. Measuring Parallel
            Performance](performance.html){.nav-link}
            -   [14.4.1. Parallel Performance
                Basics](performance_basics.html){.nav-link}
            -   [14.4.2. Advanced
                Topics](performance_advanced.html){.nav-link}
        -   [14.5. Cache Coherence](cache_coherence.html){.nav-link}
        -   [14.6. Thread Safety](thread_safety.html){.nav-link}
        -   [14.7. Implicit Threading with
            OpenMP](openmp.html){.nav-link}
        -   [14.8. Summary](summary.html){.nav-link}
        -   [14.9. Exercises](exercises.html){.nav-link}

-   -   [15. Looking Ahead: Other Parallel
        Systems](../C15-Parallel/index.html){.nav-link}
        -   [15.1. Hardware Acceleration and
            CUDA](../C15-Parallel/gpu.html){.nav-link}
        -   [15.2. Distributed Memory
            Systems](../C15-Parallel/distrmem.html){.nav-link}
        -   [15.3. To Exascale and
            Beyond](../C15-Parallel/cloud.html){.nav-link}

-   -   [16. Appendix 1: Chapter 1 for Java
        Programmers](../Appendix1/index.html){.nav-link}
        -   [16.1. Getting Started Programming in
            C](../Appendix1/getting_started.html){.nav-link}
        -   [16.2. Input/Output (printf and
            scanf)](../Appendix1/input_output.html){.nav-link}
        -   [16.3. Conditionals and
            Loops](../Appendix1/conditionals.html){.nav-link}
        -   [16.4. Functions](../Appendix1/functions.html){.nav-link}
        -   [16.5. Arrays and
            Strings](../Appendix1/arrays_strings.html){.nav-link}
        -   [16.6. Structs](../Appendix1/structs.html){.nav-link}
        -   [16.7. Summary](../Appendix1/summary.html){.nav-link}
        -   [16.8. Exercises](../Appendix1/exercises.html){.nav-link}

-   -   [17. Appendix 2: Using Unix](../Appendix2/index.html){.nav-link}
        -   [17.1. Unix Command Line and the Unix File
            System](../Appendix2/cmdln_basics.html){.nav-link}
        -   [17.2. Man and the Unix
            Manual](../Appendix2/man.html){.nav-link}
        -   [17.3. Remote Access](../Appendix2/ssh_scp.html){.nav-link}
        -   [17.4. Unix Editors](../Appendix2/editors.html){.nav-link}
        -   [17.5. make and
            Makefiles](../Appendix2/makefiles.html){.nav-link}
        -   [17.6 Searching: grep and
            find](../Appendix2/grep.html){.nav-link}
        -   [17.7 File Permissions](../Appendix2/chmod.html){.nav-link}
        -   [17.8 Archiving and Compressing
            Files](../Appendix2/tar.html){.nav-link}
        -   [17.9 Process Control](../Appendix2/pskill.html){.nav-link}
        -   [17.10 Timing](../Appendix2/timing.html){.nav-link}
        -   [17.11 Command
            History](../Appendix2/history.html){.nav-link}
        -   [17.12 I/0
            Redirection](../Appendix2/ioredirect.html){.nav-link}
        -   [17.13 Pipes](../Appendix2/pipe.html){.nav-link}
        -   [17.14 Dot Files and
            .bashrc](../Appendix2/dotfiles.html){.nav-link}
        -   [17.15 Shell
            Programming](../Appendix2/shellprog.html){.nav-link}
        -   [17.16 Getting System
            Information](../Appendix2/sysinfo.html){.nav-link}



-   [Dive Into Systems](../index-2.html)
-   [14. Leveraging Shared Memory in the Multicore Era](index.html)
-   [14.4. Measuring Parallel Performance](performance.html)
-   [14.4.2. Advanced Topics](performance_advanced.html)
:::

::: content
::: sect2
### [](#_advanced_topics){.anchor}14.4.2. Advanced Topics {#_advanced_topics}

::: sect3
#### [](#_gustafson_barsis_law){.anchor}Gustafson-Barsis Law {#_gustafson_barsis_law}

::: paragraph
In 1988, John L. Gustafson, a computer scientist and researcher at
Sandia National Labs, wrote a paper called \"Reevaluating Amdahl's
Law^1^\". In this paper, Gustafson calls to light a critical assumption
that was made about the execution of a parallel program that is not
always true.
:::

::: paragraph
Specifically, Amdahl's law implies that the number of compute cores *c*
and the fraction of a program that is parallelizable *P* are independent
of each other. Gustafson notes that this \"*is virtually never the
case*.\"^1^ While benchmarking a program's performance by varying the
number of cores on a fixed set of data is a useful academic exercise, in
the real world, more cores (or processors, as examined in our discussion
of distributed memory) are added as the problem grows large. \"*It may
be most realistic*,\" Gustafson writes^1^, \"*to assume run time, not
problem size, is constant*.\"
:::

::: paragraph
Thus, according to Gustafson, it is most accurate to say that \"*The
amount of work that can be done in parallel varies linearly with the
number of processors*.\"^1^
:::

::: paragraph
Consider a *parallel* program that takes time T~*c*~ to run on a system
with *c* cores. Let *S* represent the fraction of the program execution
that is necessarily serial and takes *S* × T~*c*~ time to run. Thus, the
parallelizable fraction of the program execution, *P* = 1 - *S*, takes
*P* × T~*c*~ time to run on *c* cores.
:::

::: paragraph
When the same program is run on just one core, the serial fraction of
the code still takes *S* x T~*c*~ (assuming all other conditions are
equal). However, the parallelizable fraction (which was divided between
*c* cores) now has to be executed by just one core to run serially and
takes *P* × T~*c*~ × *c* time. In other words, the parallel component
will take *c* times as long on a single-core system. It follows that the
scaled speedup would be:
:::

::: imageblock
::: content
![sspeedup](_images/sspeedup.png)
:::
:::

::: paragraph
This shows that the scaled speedup increases linearly with the number of
compute units.
:::

::: paragraph
Consider our prior example in which 99% of a program is parallelizable
(i.e., *P* = 0.99). Applying the scaled speedup equation, the
theoretical speedup on 100 processors would be 99.01. On 1,000
processors, it would be 990.01. Notice that the efficiency stays
constant at *P*.
:::

::: paragraph
As Gustafson concludes, \"*speedup should be measured by scaling the
problem to the number of processors, not by fixing a problem size*.\"^1^
Gustafson's result is notable because it shows that it is possible to
get increasing speedup by updating the number of processors. As a
researcher working in a national supercomputing facility, Gustafson was
more interested in doing *more work* in a constant amount of time. In
several scientific fields, the ability to analyze more data usually
leads to higher accuracy or fidelity of results. Gustafson's work showed
that it was possible to get large speedups on large numbers of
processors, and revived interest in parallel processing^2^.
:::
:::

::: sect3
#### [](#_scalability){.anchor}Scalability {#_scalability}

::: paragraph
We describe a program as **scalable** if we see improving (or constant)
performance as we increase the number of resources (cores, processors)
or the problem size. Two related concepts are **strong scaling** and
**weak scaling**. It is important to note that \"weak\" and \"strong\"
in this context do not indicate the *quality* of a program's
scalability, but are simply different ways to measure scalability.
:::

::: paragraph
We say that a program is **strongly scalable** if increasing the number
of cores/processing units on a *fixed* problem size yields an
improvement in performance. A program displays strong linear scalability
if, when run on *n* cores, the speedup is also *n*. Of course, Amdahl's
Law guarantees that after some point, adding additional cores makes
little sense.
:::

::: paragraph
We say that a program is **weakly scalable** if increasing the size of
the data at the same rate as the number of cores (i.e., if there is a
fixed data size per core/processor) results in constant or an
improvement in performance. We say a program displays weak linear
scalability if we see an improvement of *n* if the work per core is
scaled up by a factor of *n*.
:::
:::

::: sect3
#### [](#_general_advice_regarding_measuring_performance){.anchor}General Advice Regarding Measuring Performance {#_general_advice_regarding_measuring_performance}

::: paragraph
We conclude our discussion on performance with some notes about
benchmarking and performance on hyperthreaded cores.
:::

::: dlist

Run a program multiple times when benchmarking.

:   In many of the examples shown thus far in this book, we run a
    program only once to get a sense of its runtime. However, this is
    not sufficient for formal benchmarks. Running a program once is
    *never* an accurate measure of a program's true runtime! Context
    switches and other running processes can temporarily cause the
    runtime to radically fluctuate. Therefore, it is always best to run
    a program several times and report an average runtime together with
    as many details as feasible, including number of runs, observed
    variability of the measurements (e.g., error bars, minimum, maximum,
    median, standard deviation) and conditions under which the
    measurements were taken.

Be careful where you measure timing.

:   The `gettimeofday` function is useful in helping to accurately
    measure the time a program takes to run. However, it can also be
    abused. Even though it may be tempting to place the `gettimeofday`
    call around only the thread creation and joining component in
    `main`, it is important to consider what exactly you are trying to
    time. For example, if a program reads in an external data file as a
    necessary part of its execution, the time for file reading should
    likely be included in the program's timing.

Be aware of the impact of hyperthreaded cores.

:   As discussed in the [introduction to this
    chapter](index.html#_taking_a_closer_look_how_many_cores){.page} and
    [hardware multithreading
    section](../C5-Arch/modern.html#_multicore_and_hardware_multithreading){.page},
    hyper-threaded (logical) cores are capable of executing multiple
    threads on a single core. In a quad-core system with two logical
    threads per core, we say there are eight hyperthreaded cores on the
    system. Running a program in parallel on eight logical cores in many
    cases yields better wall time than running a program on four cores.
    However, due to the resource contention that usually occurs with
    hyperthreaded cores, you may see a dip in core efficiency and
    nonlinear speedup.

Beware of resource contention.

:   When benchmarking, it's always important to consider what *other*
    processes and threaded applications are running on the system. If
    your performance results ever look a bit strange, it is worth
    quickly running `top` to see whether there are any other users also
    running resource-intensive tasks on the same system. If so, try
    using a different system to benchmark (or wait until the system is
    not so heavily used).
:::
:::

::: sect3
#### [](#_references){.anchor}References {#_references}

::: {.olist .arabic}
1.  John Gustafson. \"Reevaluating Amdahl's law\". *Communications of
    the ACM* 31(5), pp. 532---​533. ACM. 1988.

2.  Caroline Connor. \"Movers and Shakers in HPC: John Gustafson\" *HPC
    Wire*.
    [http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html](http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html){.bare}
:::
:::
:::

::: toc-menu
:::
:::
:::
:::

Copyright (C) 2020 Dive into Systems, LLC.

*Dive into Systems,* is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0
International](https://creativecommons.org/licenses/by-nc-nd/4.0/) (CC
BY-NC-ND 4.0).
