
-   -   [11. Storage and the Memory
        Hierarchy](../C11-MemHierarchy/index.html){.nav-link}
        -   [11.1. The Memory
            Hierarchy](../C11-MemHierarchy/mem_hierarchy.html){.nav-link}
        -   [11.2. Storage
            Devices](../C11-MemHierarchy/devices.html){.nav-link}
        -   [11.3.
            Locality](../C11-MemHierarchy/locality.html){.nav-link}
        -   [11.4. Caching](../C11-MemHierarchy/caching.html){.nav-link}
        -   [11.5. Cache Analysis and
            Cachegrind](../C11-MemHierarchy/cachegrind.html){.nav-link}
        -   [11.6. Looking Ahead: Caching on Multicore
            Processors](../C11-MemHierarchy/coherency.html){.nav-link}
        -   [11.7. Summary](../C11-MemHierarchy/summary.html){.nav-link}
        -   [11.8.
            Exercises](../C11-MemHierarchy/exercises.html){.nav-link}

-   -   [12. Code Optimization](../C12-CodeOpt/index.html){.nav-link}
        -   [12.1. First Steps](../C12-CodeOpt/basic.html){.nav-link}
        -   [12.2. Other Compiler
            Optimizations](../C12-CodeOpt/loops_functions.html){.nav-link}
        -   [12.3. Memory
            Considerations](../C12-CodeOpt/memory_considerations.html){.nav-link}
        -   [12.4. Summary](../C12-CodeOpt/summary.html){.nav-link}

-   -   [13. The Operating System](../C13-OS/index.html){.nav-link}
        -   [13.1. Booting and Running](../C13-OS/impl.html){.nav-link}
        -   [13.2. Processes](../C13-OS/processes.html){.nav-link}
        -   [13.3. Virtual Memory](../C13-OS/vm.html){.nav-link}
        -   [13.4. Interprocess
            Communication](../C13-OS/ipc.html){.nav-link}
            -   [13.4.1. Signals](../C13-OS/ipc_signals.html){.nav-link}
            -   [13.4.2. Message
                Passing](../C13-OS/ipc_msging.html){.nav-link}
            -   [13.4.3. Shared
                Memory](../C13-OS/ipc_shm.html){.nav-link}
        -   [13.5. Summary and Other OS
            Functionality](../C13-OS/advanced.html){.nav-link}
        -   [13.6. Exercises](../C13-OS/exercises.html){.nav-link}

-   -   [14. Leveraging Shared Memory in the Multicore
        Era](index.html){.nav-link}
        -   [14.1. Programming Multicore
            Systems](multicore.html){.nav-link}
        -   [14.2. POSIX Threads](posix.html){.nav-link}
        -   [14.3. Synchronizing
            Threads](synchronization.html){.nav-link}
            -   [14.3.1. Mutual Exclusion](mutex.html){.nav-link}
            -   [14.3.2. Semaphores](semaphores.html){.nav-link}
            -   [14.3.3. Other Synchronization
                Constructs](other_syncs.html){.nav-link}
        -   [14.4. Measuring Parallel
            Performance](performance.html){.nav-link}
            -   [14.4.1. Parallel Performance
                Basics](performance_basics.html){.nav-link}
            -   [14.4.2. Advanced
                Topics](performance_advanced.html){.nav-link}
        -   [14.5. Cache Coherence](cache_coherence.html){.nav-link}
        -   [14.6. Thread Safety](thread_safety.html){.nav-link}
        -   [14.7. Implicit Threading with
            OpenMP](openmp.html){.nav-link}
        -   [14.8. Summary](summary.html){.nav-link}
        -   [14.9. Exercises](exercises.html){.nav-link}

-   -   [15. Looking Ahead: Other Parallel
        Systems](../C15-Parallel/index.html){.nav-link}
        -   [15.1. Hardware Acceleration and
            CUDA](../C15-Parallel/gpu.html){.nav-link}
        -   [15.2. Distributed Memory
            Systems](../C15-Parallel/distrmem.html){.nav-link}
        -   [15.3. To Exascale and
            Beyond](../C15-Parallel/cloud.html){.nav-link}

-   -   [16. Appendix 1: Chapter 1 for Java
        Programmers](../Appendix1/index.html){.nav-link}
        -   [16.1. Getting Started Programming in
            C](../Appendix1/getting_started.html){.nav-link}
        -   [16.2. Input/Output (printf and
            scanf)](../Appendix1/input_output.html){.nav-link}
        -   [16.3. Conditionals and
            Loops](../Appendix1/conditionals.html){.nav-link}
        -   [16.4. Functions](../Appendix1/functions.html){.nav-link}
        -   [16.5. Arrays and
            Strings](../Appendix1/arrays_strings.html){.nav-link}
        -   [16.6. Structs](../Appendix1/structs.html){.nav-link}
        -   [16.7. Summary](../Appendix1/summary.html){.nav-link}
        -   [16.8. Exercises](../Appendix1/exercises.html){.nav-link}

-   -   [17. Appendix 2: Using Unix](../Appendix2/index.html){.nav-link}
        -   [17.1. Unix Command Line and the Unix File
            System](../Appendix2/cmdln_basics.html){.nav-link}
        -   [17.2. Man and the Unix
            Manual](../Appendix2/man.html){.nav-link}
        -   [17.3. Remote Access](../Appendix2/ssh_scp.html){.nav-link}
        -   [17.4. Unix Editors](../Appendix2/editors.html){.nav-link}
        -   [17.5. make and
            Makefiles](../Appendix2/makefiles.html){.nav-link}
        -   [17.6 Searching: grep and
            find](../Appendix2/grep.html){.nav-link}
        -   [17.7 File Permissions](../Appendix2/chmod.html){.nav-link}
        -   [17.8 Archiving and Compressing
            Files](../Appendix2/tar.html){.nav-link}
        -   [17.9 Process Control](../Appendix2/pskill.html){.nav-link}
        -   [17.10 Timing](../Appendix2/timing.html){.nav-link}
        -   [17.11 Command
            History](../Appendix2/history.html){.nav-link}
        -   [17.12 I/0
            Redirection](../Appendix2/ioredirect.html){.nav-link}
        -   [17.13 Pipes](../Appendix2/pipe.html){.nav-link}
        -   [17.14 Dot Files and
            .bashrc](../Appendix2/dotfiles.html){.nav-link}
        -   [17.15 Shell
            Programming](../Appendix2/shellprog.html){.nav-link}
        -   [17.16 Getting System
            Information](../Appendix2/sysinfo.html){.nav-link}



-   [Dive Into Systems](../index-2.html)
-   [14. Leveraging Shared Memory in the Multicore Era](index.html)
:::

::: content
::: sect1
## [](#_leveraging_shared_memory_in_the_multicore_era){.anchor}14. Leveraging Shared Memory in the Multicore Era {#_leveraging_shared_memory_in_the_multicore_era}

::: sectionbody
::: paragraph
*The world is changed.*
:::

::: paragraph
*I feel it in the silica.*
:::

::: paragraph
*I feel it in the transistor.*
:::

::: paragraph
*I see it in the core.*
:::

::: paragraph
\~ With apologies to Galadriel (*Lord of the Rings: Fellowship of the
Ring*)
:::

::: paragraph
Until now, our discussion of architecture has focused on a purely
single-CPU world. But the world has changed. Today's CPUs have multiple
**cores**, or compute units. In this chapter, we discuss multicore
architectures, and how to leverage them to speed up the execution of
programs.
:::

::: {.admonitionblock .note}
+-----------------------------------+-----------------------------------+
|                                   | ::: title                         |
|                                   | CPUs, Processors, and Cores       |
|                                   | :::                               |
|                                   |                                   |
|                                   | ::: paragraph                     |
|                                   | In many instances in this         |
|                                   | chapter, the terms *processor*    |
|                                   | and *CPU* are used                |
|                                   | interchangeably. At a fundamental |
|                                   | level, a **processor** is any     |
|                                   | circuit that performs some        |
|                                   | computation on external data.     |
|                                   | Based on this definition, the     |
|                                   | **central processing unit** (CPU) |
|                                   | is an example of a processor. A   |
|                                   | processor or a CPU with multiple  |
|                                   | compute cores is referred to as a |
|                                   | **multicore processor** or a      |
|                                   | **multicore CPU**. A **core** is  |
|                                   | a compute unit that contains many |
|                                   | of the components that make up    |
|                                   | the classical CPU: an ALU,        |
|                                   | registers, and a bit of cache.    |
|                                   | Although a *core* is different    |
|                                   | from a processor, it is not       |
|                                   | unusual to see these terms used   |
|                                   | interchangeably in the literature |
|                                   | (especially if the literature     |
|                                   | originated at a time when         |
|                                   | multicore processors were still   |
|                                   | considered novel).                |
|                                   | :::                               |
+-----------------------------------+-----------------------------------+
:::

::: paragraph
In 1965, the founder of Intel, Gordon Moore, estimated that the number
of transistors in an integrated circuit would double every year. His
prediction, now known as **Moore's Law**, was later revised to
transistor counts doubling every *two* years. Despite the evolution of
electronic switches from Bardeen's transistor to the tiny chip
transistors that are currently used in modern computers, Moore's Law has
held true for the past 50 years. However, the turn of the millennium saw
processor design hit several critical performance walls:
:::

::: ulist
-   The **memory wall**: Improvements in memory technology did not keep
    pace with improvements in clock speed, resulting in memory becoming
    a bottleneck to performance. As a result, continuously speeding up
    the execution of a CPU no longer improves its overall system
    performance.

-   The **power wall**: Increasing the number of transistors on a
    processor necessarily increases that processor's temperature and
    power consumption, which in turn increases the required cost to
    power and cool the system. With the proliferation of multicore
    systems, power is now the dominant concern in computer system
    design.
:::

::: paragraph
The power and memory walls caused computer architects to change the way
they designed processors. Instead of adding more transistors to increase
the speed at which a CPU executes a single stream of instructions,
architects began adding multiple **compute cores** to a CPU. Compute
cores are simplified processing units that contain fewer transistors
than traditional CPUs and are generally easier to create. Combining
multiple cores on one CPU allows the CPU to execute *multiple*
independent streams of instructions at once.
:::

::: {.admonitionblock .warning}
+-----------------------------------+-----------------------------------+
|                                   | ::: title                         |
|                                   | More cores != better              |
|                                   | :::                               |
|                                   |                                   |
|                                   | ::: paragraph                     |
|                                   | It may be tempting to assume that |
|                                   | all cores are equal and that the  |
|                                   | more cores a computer has, the    |
|                                   | better it is. This is not         |
|                                   | necessarily the case! For         |
|                                   | example, **graphics processing    |
|                                   | unit** (GPU) cores have even      |
|                                   | fewer transistors than CPU cores, |
|                                   | and are specialized for           |
|                                   | particular tasks involving        |
|                                   | vectors. A typical GPU can have   |
|                                   | 5,000 or more GPU cores. However, |
|                                   | GPU cores are limited in the      |
|                                   | types of operations that they can |
|                                   | perform and are not always        |
|                                   | suitable for general-purpose      |
|                                   | computing like the CPU core.      |
|                                   | Computing with GPUs is known as   |
|                                   | **manycore** computing. In this   |
|                                   | chapter, we concentrate on        |
|                                   | **multicore** computing. See      |
|                                   | [Chapter                          |
|                                   | 15](../C1                         |
|                                   | 5-Parallel/gpu.html#_GPUs){.page} |
|                                   | for a discussion of manycore      |
|                                   | computing.                        |
|                                   | :::                               |
+-----------------------------------+-----------------------------------+
:::

### Taking a Closer Look: How Many Cores? {#_taking_a_closer_look_how_many_cores .discrete}

::: paragraph
Almost all modern computer systems have multiple cores, including small
devices like the [Raspberry Pi](https://www.raspberrypi.org/).
Identifying the number of cores on a system is critical for accurately
measuring the performance of multicore programs. On Linux and macOS
computers, the `lscpu` command provides a summary of a system's
architecture. In the following example, we show the output of the
`lscpu` command when run on a sample machine (some output is omitted to
emphasize the key features):
:::

::: listingblock
::: content
    $ lscpu

    Architecture:          x86_64
    CPU op-mode(s):        32-bit, 64-bit
    Byte Order:            Little Endian
    CPU(s):                8
    On-line CPU(s) list:   0-7
    Thread(s) per core:    2
    Core(s) per socket:    4
    Socket(s):             1
    Model name:            Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz
    CPU MHz:               1607.562
    CPU max MHz:           3900.0000
    CPU min MHz:           1600.0000
    L1d cache:             32K
    L1i cache:             32K
    L2 cache:              256K
    L3 cache:              8192K
    ...
:::
:::

::: paragraph
The `lscpu` command gives a lot of useful information, including the
type of processors, the core speed, and the number of cores. To
calculate the number of **physical** (or actual) cores on a system,
multiply the number of sockets by the number of cores per socket. The
sample `lscpu` output shown above shows that the system has one socket
with four cores per socket, or four physical cores in total.
:::

::: sidebarblock
::: content
::: title
Hyperthreading
:::

::: paragraph
At first glance, it may appear that the system in the previous example
has eight cores in total. After all, this is what the \"CPU(s)\" field
seems to imply. However, that field actually indicates the number of
**hyperthreaded** (logical) cores, not the number of physical cores.
Hyperthreading, or simultaneous multithreading (SMT), enables the
efficient processing of multiple threads on a single core. Although
hyperthreading can decrease the overall runtime of a program,
performance on hyperthreaded cores does not scale at the same rate as on
physical cores. However, if one task idles (e.g., due to a [control
hazard](../C5-Arch/pipelining_advanced.html#_pipelining_hazards_control_hazards){.page}),
another task can still utilize the core. In short, hyperthreading was
introduced to improve *process throughput* (which measures the number of
processes that complete in a given unit of time) rather than *process
speedup* (which measures the amount of runtime improvement of an
individual process). Much of our discussion of performance in the coming
chapter will focus on speedup.
:::
:::
:::

::: sidebarblock
::: content
::: title
Performance Cores and Efficiency Cores
:::

::: paragraph
On some newer architectures (such as Intel's 12th generation processors
and newer), multiplying the number of sockets by the numbers of cores
and hardware threads yields a different (usually smaller) number than
what is shown in the \"CPU(s)\" field. What's going on here? The answer
lies in the new heterogeneous architectures being developed by chip
makers. For example, starting with its 12th generation processors, Intel
has introduced an architecture that consists of a mixture of
\"Performance\" cores (**P-cores**) and \"Efficiency\" cores
(**E-cores**). The goal of this hybrid design is to delegate smaller
background tasks to the smaller power-sipping E-cores, freeing the
larger power-hungry P-cores for compute-intensive tasks. A similar
principle drives the design of the earlier big.LITTLE mobile
architecture introduced by Arm. On heterogeneous architectures, the
default output of `lscpu` only displays the available P-cores; the
number of E-cores can typically be computed by subtracting the number of
P-cores from the total cores shown in the \"CPU(s)\" field. Invoking the
`lscpu` command with its `--all` and `--extended` flags will display a
full mapping of the P-cores and E-cores on a system, with the E-core
identifiable by their lower processor speeds.
:::
:::
:::
:::
:::

::: toc-menu
:::
:::
:::
:::

Copyright (C) 2020 Dive into Systems, LLC.

*Dive into Systems,* is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0
International](https://creativecommons.org/licenses/by-nc-nd/4.0/) (CC
BY-NC-ND 4.0).
