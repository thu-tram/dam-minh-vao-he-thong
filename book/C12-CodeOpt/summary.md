
-   -   [11. Storage and the Memory
        Hierarchy](../C11-MemHierarchy/index.html){.nav-link}
        -   [11.1. The Memory
            Hierarchy](../C11-MemHierarchy/mem_hierarchy.html){.nav-link}
        -   [11.2. Storage
            Devices](../C11-MemHierarchy/devices.html){.nav-link}
        -   [11.3.
            Locality](../C11-MemHierarchy/locality.html){.nav-link}
        -   [11.4. Caching](../C11-MemHierarchy/caching.html){.nav-link}
        -   [11.5. Cache Analysis and
            Cachegrind](../C11-MemHierarchy/cachegrind.html){.nav-link}
        -   [11.6. Looking Ahead: Caching on Multicore
            Processors](../C11-MemHierarchy/coherency.html){.nav-link}
        -   [11.7. Summary](../C11-MemHierarchy/summary.html){.nav-link}
        -   [11.8.
            Exercises](../C11-MemHierarchy/exercises.html){.nav-link}

-   -   [12. Code Optimization](index.html){.nav-link}
        -   [12.1. First Steps](basic.html){.nav-link}
        -   [12.2. Other Compiler
            Optimizations](loops_functions.html){.nav-link}
        -   [12.3. Memory
            Considerations](memory_considerations.html){.nav-link}
        -   [12.4. Summary](summary.html){.nav-link}

-   -   [13. The Operating System](../C13-OS/index.html){.nav-link}
        -   [13.1. Booting and Running](../C13-OS/impl.html){.nav-link}
        -   [13.2. Processes](../C13-OS/processes.html){.nav-link}
        -   [13.3. Virtual Memory](../C13-OS/vm.html){.nav-link}
        -   [13.4. Interprocess
            Communication](../C13-OS/ipc.html){.nav-link}
            -   [13.4.1. Signals](../C13-OS/ipc_signals.html){.nav-link}
            -   [13.4.2. Message
                Passing](../C13-OS/ipc_msging.html){.nav-link}
            -   [13.4.3. Shared
                Memory](../C13-OS/ipc_shm.html){.nav-link}
        -   [13.5. Summary and Other OS
            Functionality](../C13-OS/advanced.html){.nav-link}
        -   [13.6. Exercises](../C13-OS/exercises.html){.nav-link}

-   -   [14. Leveraging Shared Memory in the Multicore
        Era](../C14-SharedMemory/index.html){.nav-link}
        -   [14.1. Programming Multicore
            Systems](../C14-SharedMemory/multicore.html){.nav-link}
        -   [14.2. POSIX
            Threads](../C14-SharedMemory/posix.html){.nav-link}
        -   [14.3. Synchronizing
            Threads](../C14-SharedMemory/synchronization.html){.nav-link}
            -   [14.3.1. Mutual
                Exclusion](../C14-SharedMemory/mutex.html){.nav-link}
            -   [14.3.2.
                Semaphores](../C14-SharedMemory/semaphores.html){.nav-link}
            -   [14.3.3. Other Synchronization
                Constructs](../C14-SharedMemory/other_syncs.html){.nav-link}
        -   [14.4. Measuring Parallel
            Performance](../C14-SharedMemory/performance.html){.nav-link}
            -   [14.4.1. Parallel Performance
                Basics](../C14-SharedMemory/performance_basics.html){.nav-link}
            -   [14.4.2. Advanced
                Topics](../C14-SharedMemory/performance_advanced.html){.nav-link}
        -   [14.5. Cache
            Coherence](../C14-SharedMemory/cache_coherence.html){.nav-link}
        -   [14.6. Thread
            Safety](../C14-SharedMemory/thread_safety.html){.nav-link}
        -   [14.7. Implicit Threading with
            OpenMP](../C14-SharedMemory/openmp.html){.nav-link}
        -   [14.8. Summary](../C14-SharedMemory/summary.html){.nav-link}
        -   [14.9.
            Exercises](../C14-SharedMemory/exercises.html){.nav-link}

-   -   [15. Looking Ahead: Other Parallel
        Systems](../C15-Parallel/index.html){.nav-link}
        -   [15.1. Hardware Acceleration and
            CUDA](../C15-Parallel/gpu.html){.nav-link}
        -   [15.2. Distributed Memory
            Systems](../C15-Parallel/distrmem.html){.nav-link}
        -   [15.3. To Exascale and
            Beyond](../C15-Parallel/cloud.html){.nav-link}

-   -   [16. Appendix 1: Chapter 1 for Java
        Programmers](../Appendix1/index.html){.nav-link}
        -   [16.1. Getting Started Programming in
            C](../Appendix1/getting_started.html){.nav-link}
        -   [16.2. Input/Output (printf and
            scanf)](../Appendix1/input_output.html){.nav-link}
        -   [16.3. Conditionals and
            Loops](../Appendix1/conditionals.html){.nav-link}
        -   [16.4. Functions](../Appendix1/functions.html){.nav-link}
        -   [16.5. Arrays and
            Strings](../Appendix1/arrays_strings.html){.nav-link}
        -   [16.6. Structs](../Appendix1/structs.html){.nav-link}
        -   [16.7. Summary](../Appendix1/summary.html){.nav-link}
        -   [16.8. Exercises](../Appendix1/exercises.html){.nav-link}

-   -   [17. Appendix 2: Using Unix](../Appendix2/index.html){.nav-link}
        -   [17.1. Unix Command Line and the Unix File
            System](../Appendix2/cmdln_basics.html){.nav-link}
        -   [17.2. Man and the Unix
            Manual](../Appendix2/man.html){.nav-link}
        -   [17.3. Remote Access](../Appendix2/ssh_scp.html){.nav-link}
        -   [17.4. Unix Editors](../Appendix2/editors.html){.nav-link}
        -   [17.5. make and
            Makefiles](../Appendix2/makefiles.html){.nav-link}
        -   [17.6 Searching: grep and
            find](../Appendix2/grep.html){.nav-link}
        -   [17.7 File Permissions](../Appendix2/chmod.html){.nav-link}
        -   [17.8 Archiving and Compressing
            Files](../Appendix2/tar.html){.nav-link}
        -   [17.9 Process Control](../Appendix2/pskill.html){.nav-link}
        -   [17.10 Timing](../Appendix2/timing.html){.nav-link}
        -   [17.11 Command
            History](../Appendix2/history.html){.nav-link}
        -   [17.12 I/0
            Redirection](../Appendix2/ioredirect.html){.nav-link}
        -   [17.13 Pipes](../Appendix2/pipe.html){.nav-link}
        -   [17.14 Dot Files and
            .bashrc](../Appendix2/dotfiles.html){.nav-link}
        -   [17.15 Shell
            Programming](../Appendix2/shellprog.html){.nav-link}
        -   [17.16 Getting System
            Information](../Appendix2/sysinfo.html){.nav-link}



-   [Dive Into Systems](../index-2.html)
-   [12. Code Optimization](index.html)
-   [12.4. Summary](summary.html)
:::

::: content
::: sect1
## [](#_key_takeaways_and_summary){.anchor}12.4. Key Takeaways and Summary {#_key_takeaways_and_summary}

::: sectionbody
::: paragraph
Our short (and perhaps frustrating) journey into code optimization
should convey one very important message to the reader: if you are
thinking about manually optimizing your code, think carefully about what
is worth spending your time on and what should be left to the compiler.
Below are some important tips to consider when looking to improve code
performance.
:::

::: dlist

Choose Good Data Structures and Algorithms

:   There is no substitute for using proper algorithms and data
    structures; failure to do so is often the top reason for poor
    performance in code. For example, the famous Sieve of Eratosthenes
    algorithm is a much more efficient way to generate prime numbers
    than our custom algorithm in `optExample`, and yields a significant
    improvement in performance. The following listing shows the time
    needed to generate all prime numbers between 2 and 5 million using
    an implementation of the sieve:
:::

::: listingblock
::: content
``` {.highlightjs .highlight}
$ gcc -o genPrimes genPrimes.c
$ ./genPrimes 5000000
Found 348513 primes (0.122245 s)
```
:::
:::

::: paragraph
The sieve algorithm requires only 0.12 seconds to find all the prime
numbers between 2 and 5 million, compared to the 1.46 seconds it takes
`optExample2` to generate the same set of primes with the `-O3`
optimization flags turned on (12Ã— improvement). The implementation of
the sieve algorithm is left as an exercise for the reader; however, it
should be clear that choosing a better algorithm up front would have
saved hours of tedious optimization effort. Our example demonstrates why
a knowledge of data structures and algorithms is foundational for
computer scientists.
:::

::: dlist

Use Standard Library Functions Whenever Possible

:   Don't reinvent the wheel. If in the course of programming you need a
    function that should do something very standard (e.g., find the
    absolute value, or find the maximum or minimum of a list of
    numbers), stop and check to see whether the function already exists
    as part of the higher-level language's standard library. Functions
    in the standard libraries are well tested and tend to be optimized
    for performance. For example, if a reader manually implements their
    own version of the `sqrt` function, the compiler may not know to
    automatically replace the function call with the `fsqrt`
    instruction.

Optimize Based on Data and Not on Feelings

:   If after choosing the best data structures and algorithms *and*
    employing standard library functions, additional improvements in
    performance are required, enlist the help of a good code profiler
    like Valgrind. Optimization should *never* be based on gut feelings.
    Concentrating too much on what one *feels* should be optimized
    (without the data to back up the thought) often leads to wasted
    time.

Split Complex Code into Multiple Functions

:   Manually inlining code usually does not result in a sizable
    performance gain over what modern compilers can achieve. Instead,
    make it easier for your compiler to help optimize for you. Compilers
    have an easier time optimizing shorter code segments. Splitting
    complex operations into multiple functions simultaneously increases
    code readability and makes it easier for a compiler to optimize.
    Check to see whether your compiler attempts inlining by default or
    has a separate flag to attempt inlining code. It is better to let
    your compiler perform inlining rather than manually doing it
    yourself.

Prioritize Code Readability

:   In many applications today, readability is king. The truth is that
    code is read more often than it is written. Many companies spend
    considerable time training their software engineers to write code in
    a very particular way to maximize readability. If optimizing your
    code results in a noticeable hit to code readability, it is
    important to check if the performance improvement obtained is worth
    the hit. For example, many compilers today have optimization flags
    that enable loop unrolling. Programmers should always use available
    optimization flags for loop unrolling instead of trying to manually
    unroll loops, which can lead to a significant hit in code
    readability. Reducing code readability often increases the
    likelihood that bugs are inadvertently introduced into code, which
    can lead to security vulnerabilities.

Pay Attention to Memory Use

:   A program's memory usage often has a bigger impact on the program's
    execution time than the number of instructions that it executes. The
    [loop interchange
    example](memory_considerations.html#_loop_interchange){.page}
    exemplifies this point. In both cases, the loop executes the same
    number of instructions. However, the ordering of the loops has a
    significant impact on memory access and locality. Remember to also
    explore memory profiling tools like `massif` and `cachegrind` when
    attempting to optimize a program.

Compilers Are Constantly Improving

:   Compiler writers continually update compilers to perform more
    sophisticated optimizations safely. For example, GCC switched to the
    link: [static single assignment
    (SSA)](https://gcc.gnu.org/onlinedocs/gccint/SSA.html) form starting
    in version 4.0, which significantly improved the effects of some of
    its optimizations. The `GRAPHITE` branch of the GCC code base
    implements the [polyhedral model](https://polyhedral.info/), which
    allows the compiler to perform more complex types of loop
    transformations. As compilers get more sophisticated, the benefits
    of manual optimization significantly reduce.
:::
:::
:::

::: toc-menu
:::
:::
:::
:::

Copyright (C) 2020 Dive into Systems, LLC.

*Dive into Systems,* is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0
International](https://creativecommons.org/licenses/by-nc-nd/4.0/) (CC
BY-NC-ND 4.0).
