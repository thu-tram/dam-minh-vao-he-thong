
-   -   [11. Storage and the Memory Hierarchy](index.html){.nav-link}
        -   [11.1. The Memory Hierarchy](mem_hierarchy.html){.nav-link}
        -   [11.2. Storage Devices](devices.html){.nav-link}
        -   [11.3. Locality](locality.html){.nav-link}
        -   [11.4. Caching](caching.html){.nav-link}
        -   [11.5. Cache Analysis and
            Cachegrind](cachegrind.html){.nav-link}
        -   [11.6. Looking Ahead: Caching on Multicore
            Processors](coherency.html){.nav-link}
        -   [11.7. Summary](summary.html){.nav-link}
        -   [11.8. Exercises](exercises.html){.nav-link}

-   -   [12. Code Optimization](../C12-CodeOpt/index.html){.nav-link}
        -   [12.1. First Steps](../C12-CodeOpt/basic.html){.nav-link}
        -   [12.2. Other Compiler
            Optimizations](../C12-CodeOpt/loops_functions.html){.nav-link}
        -   [12.3. Memory
            Considerations](../C12-CodeOpt/memory_considerations.html){.nav-link}
        -   [12.4. Summary](../C12-CodeOpt/summary.html){.nav-link}

-   -   [13. The Operating System](../C13-OS/index.html){.nav-link}
        -   [13.1. Booting and Running](../C13-OS/impl.html){.nav-link}
        -   [13.2. Processes](../C13-OS/processes.html){.nav-link}
        -   [13.3. Virtual Memory](../C13-OS/vm.html){.nav-link}
        -   [13.4. Interprocess
            Communication](../C13-OS/ipc.html){.nav-link}
            -   [13.4.1. Signals](../C13-OS/ipc_signals.html){.nav-link}
            -   [13.4.2. Message
                Passing](../C13-OS/ipc_msging.html){.nav-link}
            -   [13.4.3. Shared
                Memory](../C13-OS/ipc_shm.html){.nav-link}
        -   [13.5. Summary and Other OS
            Functionality](../C13-OS/advanced.html){.nav-link}
        -   [13.6. Exercises](../C13-OS/exercises.html){.nav-link}

-   -   [14. Leveraging Shared Memory in the Multicore
        Era](../C14-SharedMemory/index.html){.nav-link}
        -   [14.1. Programming Multicore
            Systems](../C14-SharedMemory/multicore.html){.nav-link}
        -   [14.2. POSIX
            Threads](../C14-SharedMemory/posix.html){.nav-link}
        -   [14.3. Synchronizing
            Threads](../C14-SharedMemory/synchronization.html){.nav-link}
            -   [14.3.1. Mutual
                Exclusion](../C14-SharedMemory/mutex.html){.nav-link}
            -   [14.3.2.
                Semaphores](../C14-SharedMemory/semaphores.html){.nav-link}
            -   [14.3.3. Other Synchronization
                Constructs](../C14-SharedMemory/other_syncs.html){.nav-link}
        -   [14.4. Measuring Parallel
            Performance](../C14-SharedMemory/performance.html){.nav-link}
            -   [14.4.1. Parallel Performance
                Basics](../C14-SharedMemory/performance_basics.html){.nav-link}
            -   [14.4.2. Advanced
                Topics](../C14-SharedMemory/performance_advanced.html){.nav-link}
        -   [14.5. Cache
            Coherence](../C14-SharedMemory/cache_coherence.html){.nav-link}
        -   [14.6. Thread
            Safety](../C14-SharedMemory/thread_safety.html){.nav-link}
        -   [14.7. Implicit Threading with
            OpenMP](../C14-SharedMemory/openmp.html){.nav-link}
        -   [14.8. Summary](../C14-SharedMemory/summary.html){.nav-link}
        -   [14.9.
            Exercises](../C14-SharedMemory/exercises.html){.nav-link}

-   -   [15. Looking Ahead: Other Parallel
        Systems](../C15-Parallel/index.html){.nav-link}
        -   [15.1. Hardware Acceleration and
            CUDA](../C15-Parallel/gpu.html){.nav-link}
        -   [15.2. Distributed Memory
            Systems](../C15-Parallel/distrmem.html){.nav-link}
        -   [15.3. To Exascale and
            Beyond](../C15-Parallel/cloud.html){.nav-link}

-   -   [16. Appendix 1: Chapter 1 for Java
        Programmers](../Appendix1/index.html){.nav-link}
        -   [16.1. Getting Started Programming in
            C](../Appendix1/getting_started.html){.nav-link}
        -   [16.2. Input/Output (printf and
            scanf)](../Appendix1/input_output.html){.nav-link}
        -   [16.3. Conditionals and
            Loops](../Appendix1/conditionals.html){.nav-link}
        -   [16.4. Functions](../Appendix1/functions.html){.nav-link}
        -   [16.5. Arrays and
            Strings](../Appendix1/arrays_strings.html){.nav-link}
        -   [16.6. Structs](../Appendix1/structs.html){.nav-link}
        -   [16.7. Summary](../Appendix1/summary.html){.nav-link}
        -   [16.8. Exercises](../Appendix1/exercises.html){.nav-link}

-   -   [17. Appendix 2: Using Unix](../Appendix2/index.html){.nav-link}
        -   [17.1. Unix Command Line and the Unix File
            System](../Appendix2/cmdln_basics.html){.nav-link}
        -   [17.2. Man and the Unix
            Manual](../Appendix2/man.html){.nav-link}
        -   [17.3. Remote Access](../Appendix2/ssh_scp.html){.nav-link}
        -   [17.4. Unix Editors](../Appendix2/editors.html){.nav-link}
        -   [17.5. make and
            Makefiles](../Appendix2/makefiles.html){.nav-link}
        -   [17.6 Searching: grep and
            find](../Appendix2/grep.html){.nav-link}
        -   [17.7 File Permissions](../Appendix2/chmod.html){.nav-link}
        -   [17.8 Archiving and Compressing
            Files](../Appendix2/tar.html){.nav-link}
        -   [17.9 Process Control](../Appendix2/pskill.html){.nav-link}
        -   [17.10 Timing](../Appendix2/timing.html){.nav-link}
        -   [17.11 Command
            History](../Appendix2/history.html){.nav-link}
        -   [17.12 I/0
            Redirection](../Appendix2/ioredirect.html){.nav-link}
        -   [17.13 Pipes](../Appendix2/pipe.html){.nav-link}
        -   [17.14 Dot Files and
            .bashrc](../Appendix2/dotfiles.html){.nav-link}
        -   [17.15 Shell
            Programming](../Appendix2/shellprog.html){.nav-link}
        -   [17.16 Getting System
            Information](../Appendix2/sysinfo.html){.nav-link}



-   [Dive Into Systems](../index-2.html)
-   [11. Storage and the Memory Hierarchy](index.html)
-   [11.3. Locality](locality.html)
:::

::: content
::: sect1
## [](#_locality){.anchor}11.3. Locality {#_locality}

::: sectionbody
::: paragraph
Because [memory devices vary
considerably](mem_hierarchy.html#_the_memory_hierarchy){.page} in their
performance characteristics and storage capacities, modern systems
integrate several forms of storage. Luckily, most programs exhibit
common memory access patterns, known as **locality**, and designers
build hardware that exploits good locality to automatically move data
into an appropriate storage location. Specifically, a system improves
performance by moving the subset of data that a program is actively
using into storage that lives close to the CPU's computation circuitry
(for example, in a register or CPU cache). As necessary data moves up
the hierarchy toward the CPU, unused data moves farther away to slower
storage until the program needs it.
:::

::: paragraph
To a system designer, building a system that exploits locality
represents an abstraction problem. The system provides an abstract view
of memory devices such that it appears to programmers as if they have
the sum of all memory capacities with the performance characteristics of
fast on-chip storage. Of course, providing this rosy illusion to users
can't be accomplished perfectly, but by exploiting program locality,
modern systems achieve good performance for most well-written programs.
:::

::: paragraph
Systems primarily exploit two forms of locality:
:::

::: {.olist .arabic}
1.  **Temporal locality:** Programs tend to access the same data
    repeatedly over time. That is, if a program has used a variable
    recently, it's likely to use that variable again soon.

2.  **Spatial locality:** Programs tend to access data that is nearby
    other, previously accessed data. \"Nearby\" here refers to the
    data's memory address. For example, if a program accesses data at
    addresses *N* and *N+4*, it's likely to access *N+8* soon.
:::

::: sect2
### [](#_locality_examples_in_code){.anchor}11.3.1. Locality Examples in Code {#_locality_examples_in_code}

::: paragraph
Fortunately, common programming patterns exhibit both forms of locality
quite frequently. Take the following function, for example:
:::

::: listingblock
::: content
``` {.highlightjs .highlight}
/* Sum up the elements in an integer array of length len. */
int sum_array(int *array, int len) {
    int i;
    int sum = 0;

    for (i = 0; i < len; i++) {
        sum += array[i];
    }

    return sum;
}
```
:::
:::

::: paragraph
In this code, the repetitive nature of the `for` loop introduces
temporal locality for `i`, `len`, `sum`, and `array` (the base address
of the array), as the program accesses each of these variables within
every loop iteration. Exploiting this temporal locality allows a system
to load each variable from main memory into the CPU cache only once.
Every subsequent access can be serviced out of the significantly faster
cache.
:::

::: paragraph
Accesses to the array's contents also benefit from spatial locality.
Even though the program accesses each array element only once, a modern
system loads more than one `int` at a time from memory to the CPU cache.
That is, accessing the first array index fills the cache with not only
the first integer, but also the next few integers after it, too. Exactly
*how many* additional integers get moved into the cache depends on the
cache's *block size* --- the amount of data transferred into the cache
at once.
:::

::: paragraph
For example, with a 16-byte block size, a system copies four integers
from memory to the cache at a time. Thus, accessing the first integer
incurs the relatively high cost of accessing main memory, but the
accesses to the next three are served out of cache, even if the program
has never accessed them previously.
:::

::: paragraph
In many cases, a programmer can help a system by intentionally writing
code that exhibits good locality patterns. For example, consider the
nested loops that access every element of an *N*×*N* matrix (this same
example appeared in this chapter's introduction):
:::

+-----------------------------------+-----------------------------------+
| ::: content                       | ::: content                       |
| ::: listingblock                  | ::: listingblock                  |
| ::: content                       | ::: content                       |
| ``` {.highlightjs .highlight}     | ``` {.highlightjs .highlight}     |
| float                             | float                             |
| averageMat_v1(int **mat, int n) { | averageMat_v2(int **mat, int n) { |
|     int i, j, total = 0;          |     int i, j, total = 0;          |
|                                   |                                   |
|     for (i = 0; i < n; i++) {     |     for (i = 0; i < n; i++) {     |
|         for (j = 0; j < n; j++) { |         for (j = 0; j < n; j++) { |
|                                   |                                   |
|          // Note indexing: [i][j] |          // Note indexing: [j][i] |
|             total += mat[i][j];   |             total += mat[j][i];   |
|         }                         |         }                         |
|     }                             |     }                             |
|                                   |                                   |
|                                   |                                   |
|   return (float) total / (n * n); |   return (float) total / (n * n); |
| }                                 | }                                 |
| ```                               | ```                               |
| :::                               | :::                               |
| :::                               | :::                               |
| :::                               | :::                               |
+-----------------------------------+-----------------------------------+

: Table 1. Two versions of a function that accesses every element of an
*N*x*N* matrix. They only differ in their memory indexing into the
matrix, but version 1 (left) executes approximately five times faster.

::: paragraph
In both versions, the loop variables (`i` and `j`) and the accumulator
variable (`total`) exhibit good temporal locality because the loops
repeatedly use them in every iteration. Thus, when executing this code,
a system would store those variables in fast on-CPU storage locations to
provide good performance.
:::

::: paragraph
However, due to the [*row-major order* organization of a matrix in
memory](../C2-C_depth/arrays.html#_two_dimensional_array_memory_layout){.page},
the first version of the code (left) executes about five times faster
than the second version (right). The disparity arises from the
difference in spatial locality --- the first version accesses the
matrix's values sequentially in memory (that is, in order of consecutive
memory addresses). Thus, it benefits from a system that loads large
blocks from memory into the cache because it pays the cost of going to
memory only once for every block of values.
:::

::: paragraph
The second version accesses the matrix's values by repeatedly jumping
between rows across nonsequential memory addresses. It *never* reads
from the same cache block in subsequent memory accesses, so it looks to
the cache like the block isn't needed. Thus, it pays the cost of going
to memory for every matrix value it reads.
:::

::: paragraph
This example illustrates how a programmer can affect the system-level
costs of a program's execution. Keep these principles in mind when
writing high-performance applications, particularly those that access
arrays in a regular pattern.
:::
:::

::: sect2
### [](#_from_locality_to_caches){.anchor}11.3.2. From Locality to Caches {#_from_locality_to_caches}

::: paragraph
To help illustrate how the concepts of temporal and spatial locality
enable cache designs, we'll adopt an example scenario with familiar
real-world objects: books. Suppose that Fiona does all of her homework
at a desk in her dorm room, and the desk has a small amount of space
that can store only three books. Just outside her room she keeps a
bookshelf, which has much more space than the desk. Finally, across
campus her college has a library with a huge variety of books. The
\"book storage hierarchy\" in this example might look something like
[Figure 1](#FigBookHierarchy). Given this scenario, we'll explore how
locality can help guide which storage location Fiona should use to store
her books.
:::

::: {#FigBookHierarchy .imageblock .text-center}
::: content
![In order, from (quick access, low capacity) to (slow access, high
capacity): desk, shelf,
library.](_images/BookHierarchy.png){width="700"}
:::

::: title
Figure 1. A hypothetical book storage hierarchy
:::
:::
:::

::: sect2
### [](#_temporal_locality){.anchor}11.3.3. Temporal Locality {#_temporal_locality}

::: paragraph
Temporal locality suggests that, if there's a book Fiona uses
frequently, she should keep it as close to her desk as possible. If she
occasionally needs to move it to the shelf to clear up temporary work
space, the cost isn't too high, but it would be silly to take a book
back to the library if she's just going to need it again the next day.
The inverse is also true: if there's a book taking up valuable space on
her desk or shelf, and she hasn't used it for quite a while, that book
seems like a good candidate for returning to the library.
:::

::: paragraph
So, which books should Fiona move to her precious desk space? In this
example, real students would probably look at their upcoming assignments
and select the books that they expect to be most useful. In other words,
to make the best storage decision, they would ideally need information
about *future usage*.
:::

::: paragraph
Unfortunately, hardware designers haven't discovered how to build
circuits that can predict the future. As an alternative to prediction,
one could instead imagine a system that asks the programmer or user to
inform the system in advance how a program will use data so that it's
placement could be optimized. Such a strategy may work well in
specialized applications (for example, large databases), that exhibit
*very* regular access patterns. However, in a general-purpose system
like a personal computer, requiring advance notice from the user is too
large a burden --- many users would not want to (or would be unable to)
provide enough detail to help the system make good decisions.
:::

::: paragraph
Thus, instead of relying on future access information, systems look to
the past as a predictor of what will *likely* happen in the future.
Applying this idea to the book example suggests a relatively simple (but
still quite effective) strategy for governing book storage spaces:
:::

::: ulist
-   When Fiona needs to use a book, she retrieves it from wherever it
    currently is and moves it to her desk.

-   If the desk is already full, she moves the book that she used *least
    recently* (that is, the book that has been sitting on the desk
    untouched for the longest amount of time) to her shelf.

-   If the shelf is full, she returns the shelf's least recently used
    book to the library to free up space.
:::

::: paragraph
Even though this scheme may not be perfect, the simplicity makes it
attractive. All it requires is the ability to move books between storage
locations and a small amount of metainformation regarding the order in
which books were previously used. Furthermore, this scheme captures the
two initial temporal locality objectives well:
:::

::: {.olist .arabic}
1.  Frequently used books are likely to remain on the desk or shelf,
    preventing unnecessary trips to the library.

2.  Infrequently used books eventually become the least recently used
    book, at which point returning them to the library makes sense.
:::

::: paragraph
Applying this strategy to primary storage devices looks remarkably
similar to the book example: as data is loaded into CPU registers from
main memory, make room for it in the CPU cache. If the cache is already
full, make room in the cache by **evicting** the least recently used
cache data to main memory. In the [following caching
section](caching.html#_cpu_caches){.page}, we'll explore the details of
how such mechanisms are built in to modern caching systems.
:::
:::

::: sect2
### [](#_spatial_locality){.anchor}11.3.4. Spatial Locality {#_spatial_locality}

::: paragraph
Spatial locality suggests that, when making a trip to the library, Fiona
should retrieve more than one book to reduce the likelihood of future
library trips. Specifically, she should retrieve additional books that
are \"nearby\" the one she needs, because those that are nearby seem
like good candidates for books that might otherwise turn into additional
library visits.
:::

::: paragraph
Suppose that she's taking a literature course on the topic of
Shakespeare's histories. If in the first week of the course she's
assigned to read *Henry VI, Part I*, when she finds herself in the
library to retrieve it, she's likely to also find Parts II and III close
by on the shelves. Even if she doesn't yet know whether the course will
assign those other two parts, it's not unreasonable to think that she
*might* need them. That is, the likelihood of needing them is much
higher than a random book in the library, specifically because they are
nearby the book she does need.
:::

::: paragraph
In this scenario, the likelihood increases due to the way libraries
arrange books on shelves, and programs similarly organize data in
memory. For example, a programming construct like an array or a `struct`
stores a collection of related data in a contiguous region of memory.
When iterating over consecutive elements in an array, there is clearly a
spatial pattern in the accessed memory addresses. Applying these spatial
locality lessons to primary storage devices implies that, when
retrieving data from main memory, the system should also retrieve the
data immediately surrounding it.
:::

::: paragraph
In the next section, we'll characterize cache characteristics and
describe mechanisms for the hardware to make identifying and exploiting
locality happen automatically.
:::
:::
:::
:::

::: toc-menu
:::
:::
:::
:::

Copyright (C) 2020 Dive into Systems, LLC.

*Dive into Systems,* is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0
International](https://creativecommons.org/licenses/by-nc-nd/4.0/) (CC
BY-NC-ND 4.0).
