
-   -   [11. Storage and the Memory Hierarchy](index.html){.nav-link}
        -   [11.1. The Memory Hierarchy](mem_hierarchy.html){.nav-link}
        -   [11.2. Storage Devices](devices.html){.nav-link}
        -   [11.3. Locality](locality.html){.nav-link}
        -   [11.4. Caching](caching.html){.nav-link}
        -   [11.5. Cache Analysis and
            Cachegrind](cachegrind.html){.nav-link}
        -   [11.6. Looking Ahead: Caching on Multicore
            Processors](coherency.html){.nav-link}
        -   [11.7. Summary](summary.html){.nav-link}
        -   [11.8. Exercises](exercises.html){.nav-link}

-   -   [12. Code Optimization](../C12-CodeOpt/index.html){.nav-link}
        -   [12.1. First Steps](../C12-CodeOpt/basic.html){.nav-link}
        -   [12.2. Other Compiler
            Optimizations](../C12-CodeOpt/loops_functions.html){.nav-link}
        -   [12.3. Memory
            Considerations](../C12-CodeOpt/memory_considerations.html){.nav-link}
        -   [12.4. Summary](../C12-CodeOpt/summary.html){.nav-link}

-   -   [13. The Operating System](../C13-OS/index.html){.nav-link}
        -   [13.1. Booting and Running](../C13-OS/impl.html){.nav-link}
        -   [13.2. Processes](../C13-OS/processes.html){.nav-link}
        -   [13.3. Virtual Memory](../C13-OS/vm.html){.nav-link}
        -   [13.4. Interprocess
            Communication](../C13-OS/ipc.html){.nav-link}
            -   [13.4.1. Signals](../C13-OS/ipc_signals.html){.nav-link}
            -   [13.4.2. Message
                Passing](../C13-OS/ipc_msging.html){.nav-link}
            -   [13.4.3. Shared
                Memory](../C13-OS/ipc_shm.html){.nav-link}
        -   [13.5. Summary and Other OS
            Functionality](../C13-OS/advanced.html){.nav-link}
        -   [13.6. Exercises](../C13-OS/exercises.html){.nav-link}

-   -   [14. Leveraging Shared Memory in the Multicore
        Era](../C14-SharedMemory/index.html){.nav-link}
        -   [14.1. Programming Multicore
            Systems](../C14-SharedMemory/multicore.html){.nav-link}
        -   [14.2. POSIX
            Threads](../C14-SharedMemory/posix.html){.nav-link}
        -   [14.3. Synchronizing
            Threads](../C14-SharedMemory/synchronization.html){.nav-link}
            -   [14.3.1. Mutual
                Exclusion](../C14-SharedMemory/mutex.html){.nav-link}
            -   [14.3.2.
                Semaphores](../C14-SharedMemory/semaphores.html){.nav-link}
            -   [14.3.3. Other Synchronization
                Constructs](../C14-SharedMemory/other_syncs.html){.nav-link}
        -   [14.4. Measuring Parallel
            Performance](../C14-SharedMemory/performance.html){.nav-link}
            -   [14.4.1. Parallel Performance
                Basics](../C14-SharedMemory/performance_basics.html){.nav-link}
            -   [14.4.2. Advanced
                Topics](../C14-SharedMemory/performance_advanced.html){.nav-link}
        -   [14.5. Cache
            Coherence](../C14-SharedMemory/cache_coherence.html){.nav-link}
        -   [14.6. Thread
            Safety](../C14-SharedMemory/thread_safety.html){.nav-link}
        -   [14.7. Implicit Threading with
            OpenMP](../C14-SharedMemory/openmp.html){.nav-link}
        -   [14.8. Summary](../C14-SharedMemory/summary.html){.nav-link}
        -   [14.9.
            Exercises](../C14-SharedMemory/exercises.html){.nav-link}

-   -   [15. Looking Ahead: Other Parallel
        Systems](../C15-Parallel/index.html){.nav-link}
        -   [15.1. Hardware Acceleration and
            CUDA](../C15-Parallel/gpu.html){.nav-link}
        -   [15.2. Distributed Memory
            Systems](../C15-Parallel/distrmem.html){.nav-link}
        -   [15.3. To Exascale and
            Beyond](../C15-Parallel/cloud.html){.nav-link}

-   -   [16. Appendix 1: Chapter 1 for Java
        Programmers](../Appendix1/index.html){.nav-link}
        -   [16.1. Getting Started Programming in
            C](../Appendix1/getting_started.html){.nav-link}
        -   [16.2. Input/Output (printf and
            scanf)](../Appendix1/input_output.html){.nav-link}
        -   [16.3. Conditionals and
            Loops](../Appendix1/conditionals.html){.nav-link}
        -   [16.4. Functions](../Appendix1/functions.html){.nav-link}
        -   [16.5. Arrays and
            Strings](../Appendix1/arrays_strings.html){.nav-link}
        -   [16.6. Structs](../Appendix1/structs.html){.nav-link}
        -   [16.7. Summary](../Appendix1/summary.html){.nav-link}
        -   [16.8. Exercises](../Appendix1/exercises.html){.nav-link}

-   -   [17. Appendix 2: Using Unix](../Appendix2/index.html){.nav-link}
        -   [17.1. Unix Command Line and the Unix File
            System](../Appendix2/cmdln_basics.html){.nav-link}
        -   [17.2. Man and the Unix
            Manual](../Appendix2/man.html){.nav-link}
        -   [17.3. Remote Access](../Appendix2/ssh_scp.html){.nav-link}
        -   [17.4. Unix Editors](../Appendix2/editors.html){.nav-link}
        -   [17.5. make and
            Makefiles](../Appendix2/makefiles.html){.nav-link}
        -   [17.6 Searching: grep and
            find](../Appendix2/grep.html){.nav-link}
        -   [17.7 File Permissions](../Appendix2/chmod.html){.nav-link}
        -   [17.8 Archiving and Compressing
            Files](../Appendix2/tar.html){.nav-link}
        -   [17.9 Process Control](../Appendix2/pskill.html){.nav-link}
        -   [17.10 Timing](../Appendix2/timing.html){.nav-link}
        -   [17.11 Command
            History](../Appendix2/history.html){.nav-link}
        -   [17.12 I/0
            Redirection](../Appendix2/ioredirect.html){.nav-link}
        -   [17.13 Pipes](../Appendix2/pipe.html){.nav-link}
        -   [17.14 Dot Files and
            .bashrc](../Appendix2/dotfiles.html){.nav-link}
        -   [17.15 Shell
            Programming](../Appendix2/shellprog.html){.nav-link}
        -   [17.16 Getting System
            Information](../Appendix2/sysinfo.html){.nav-link}



-   [Dive Into Systems](../index-2.html)
-   [11. Storage and the Memory Hierarchy](index.html)
-   [11.2. Storage Devices](devices.html)
:::

::: content
::: sect1
## [](#_storage_devices){.anchor}11.2. Storage Devices {#_storage_devices}

::: sectionbody
::: paragraph
Systems designers classify devices in the [memory
hierarchy](mem_hierarchy.html#_the_memory_hierarchy){.page} according to
how programs access their data. **Primary storage** devices can be
accessed directly by a program on the CPU. That is, the CPU's assembly
instructions encode the exact location of the data that the instructions
should retrieve. Examples of primary storage include CPU registers and
main memory (RAM), which assembly instructions reference directly (for
example, in IA32 assembly as `%reg` and `(%reg)`, respectively).
:::

::: paragraph
In contrast, CPU instructions cannot directly refer to **secondary
storage** devices. To access the contents of a secondary storage device,
a program must first request that the device copy its data into primary
storage (typically memory). The most familiar types of secondary storage
devices are disk devices such as hard disk drives and solid-state
drives, which persistently store file data. Other examples include
floppy disks, magnetic tape cartridges, or even remote file servers.
:::

::: paragraph
Even though you may not have considered the distinction between primary
and secondary storage in these terms before, it's likely that you have
encountered their differences in programs already. For example, after
declaring and assigning ordinary variables (primary storage), a program
can immediately use them in arithmetic operations. When working with
file data (secondary storage), the program must [read values from the
file into memory
variables](../C2-C_depth/IO.html#_file_inputoutput){.page} before it can
access them.
:::

::: paragraph
Several other important criteria for classifying memory devices arise
from their performance and capacity characteristics. The three most
interesting measures are:
:::

::: ulist
-   **Capacity**: The amount of data a device can store. Capacity is
    typically measured in bytes.

-   **Latency**: The amount of time it takes for a device to respond
    with data after it has been instructed to perform a data retrieval
    operation. Latency is typically measured in either fractions of a
    second (for example, milliseconds or nanoseconds) or CPU cycles.

-   **Transfer rate**: The amount of data that can be moved between the
    device and main memory over some interval of time. Transfer rate is
    also known as **throughput** and is typically measured in bytes per
    second.
:::

::: paragraph
Exploring the variety of devices in a modern computer reveals a huge
disparity in device performance across all three of these measures. The
performance variance primarily arises from two factors: *distance* and
*variations in the technologies* used to implement the devices.
:::

::: paragraph
Distance contributes because, ultimately, any data that a program wants
to use must be available to the CPU's arithmetic components, the ALU,
for processing. CPU designers place registers close to the ALU to
minimize the time it takes for a signal to propagate between the two.
Thus, while registers can store only a few bytes and there aren't many
of them, the values stored are available to the ALU almost immediately!
In contrast, secondary storage devices like disks transfer data to
memory through various controller devices that are connected by longer
wires. The extra distance and intermediate processing slows down
secondary storage considerably.
:::

::: sidebarblock
::: content
::: title
Grace Hopper's \"Nanoseconds\"
:::

::: paragraph
When speaking to an audience, computing pioneer and U.S. Navy Admiral
Grace Hopper frequently handed out 11.8-inch strands of wire to audience
members. These strands represented the maximum distance that an
electrical signal travels in one nanosecond and were called \"Grace
Hopper nanoseconds\". She used them to describe the latency limitations
of satellite communication and to demonstrate why computing devices need
to be small in order to be fast. Recordings of Grace Hopper presenting
her nanoseconds are [available on
YouTube](https://www.youtube.com/watch?v=9eyFDBPk4Yw).
:::
:::
:::

::: paragraph
The underlying technology also significantly affects device performance.
Registers and caches are built from relatively simple circuits,
consisting of just a few logic gates. Their small size and minimal
complexity ensures that electrical signals can propagate through them
quickly, reducing their latencies. On the opposite end of the spectrum,
traditional hard disks contain spinning magnetic platters that store
hundreds of gigabytes. Although they offer dense storage, their access
latency is relatively high due to the requirements of mechanically
aligning and rotating components into the correct positions.
:::

::: paragraph
The remainder of this section examines the details of primary and
secondary storage devices and analyzes their performance
characteristics.
:::

::: sect2
### [](#_primary_storage){.anchor}11.2.1. Primary Storage {#_primary_storage}

::: paragraph
Primary storage devices consist of **random access memory** (RAM), which
means the time it takes to access data is not affected by the data's
location in the device. That is, RAM doesn't need to worry about things
like moving parts into the correct position or rewinding tape spools.
There are two widely used types of RAM, **static RAM** (SRAM) and
**dynamic RAM** (DRAM), and both play an important role in modern
computers. [Table 1](#TabPrimaryStorage) characterizes the performance
measures of common primary storage devices and the types of RAM they
use.
:::

+-----------------+-----------------+-----------------+-----------------+
| Device          | Capacity        | Approx. latency | RAM type        |
+=================+=================+=================+=================+
| Register        | 4 - 8 bytes     | \< 1 ns         | SRAM            |
+-----------------+-----------------+-----------------+-----------------+
| CPU cache       | 1 - 32          | 5 ns            | SRAM            |
|                 | megabytes       |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| Main memory     | 4 - 64          | 100 ns          | DRAM            |
|                 | gigabytes       |                 |                 |
+-----------------+-----------------+-----------------+-----------------+

: Table 1. Primary Storage Device Characteristics of a Typical 2020
Workstation

::: paragraph
SRAM stores data in small electrical circuits (for example,
[latches](../C5-Arch/storagecircs.html#_rs_latch){.page}). SRAM is
typically the fastest type of memory, and designers integrate it
directly into a CPU to build
[registers](../C5-Arch/storagecircs.html#_cpu_register){.page} and
caches. SRAM is relatively expensive in its cost to build, cost to
operate (power consumption), and in the amount of space it occupies.
Collectively, those costs limit the amount of SRAM storage that a CPU
can include.
:::

::: paragraph
DRAM stores data using electrical components called *capacitors* that
hold an electrical charge. It's called \"dynamic\" because a DRAM system
must frequently refresh the charge of its capacitors to maintain a
stored value. Modern systems use DRAM to implement main memory on
modules that connect to the CPU via a high-speed interconnect called the
**memory bus**.
:::

::: paragraph
[Figure 1](#FigMemoryBus) illustrates the positions of primary storage
devices relative to the memory bus. To retrieve a value from memory, the
CPU puts the address of the data it would like to retrieve on the memory
bus and signals that the memory modules should perform a read. After a
short delay, the memory module sends the value stored at the requested
address across the bus to the CPU.
:::

::: {#FigMemoryBus .imageblock .text-center}
::: content
![The registers and ALU are nearby one another on the CPU. The CPU
connects to main memory via a memory bus, which consists of several
collections of wires for exchanging addresses, data, and control signals
between the CPU and memory.](_images/MemoryBus.png){width="800"}
:::

::: title
Figure 1. Primary storage and memory bus architecture
:::
:::

::: paragraph
Even though the CPU and main memory are physically just a few inches
away from each other, data must travel through the memory bus when it
moves between the CPU and main memory. The extra distance and circuitry
between them increases the latency and reduces the transfer rate of main
memory relative to on-CPU storage. As a result, the memory bus is
sometimes referred to as the **von Neumann bottleneck**. Of course,
despite its lower performance, main memory remains an essential
component because it stores several orders of magnitude more data than
can fit on the CPU. Consistent with other forms of storage, there's a
clear trade-off between capacity and speed.
:::

::: paragraph
**CPU cache** (pronounced \"cash\") occupies the middle ground between
registers and main memory, both physically and in terms of its
performance and capacity characteristics. A CPU cache typically stores a
few kilobytes to megabytes of data directly on the CPU, but physically,
caches are not quite as close to the ALU as registers. Thus, caches are
faster to access than main memory, but they require a few more cycles
than registers to make data available for computation.
:::

::: paragraph
Rather than the programmer explicitly loading values into the cache,
control circuitry within the CPU automatically stores a subset of the
main memory's contents in the cache. CPUs strategically control which
subset of main memory they store in caches so that as many memory
requests as possible can be serviced by the (much higher performance)
cache. Later sections of this chapter describe the design decisions that
go into cache construction and the algorithms that should govern which
data they store.
:::

::: paragraph
Real systems incorporate multiple levels of caches that behave like
their own miniature version of the memory hierarchy. That is, a CPU
might have a very small and fast **L1 cache** that stores a subset of a
slightly larger and slower **L2 cache**, which in turns stores a subset
of a larger and slower **L3 cache**. The remainder of this section
describes a system with just a single cache, but the interaction between
caches on a real system behaves much like the interaction between a
single cache and main memory detailed later.
:::

::: {.admonitionblock .note}
+-----------------------------------+-----------------------------------+
|                                   | ::: paragraph                     |
|                                   | If you're curious about the sizes |
|                                   | of the caches and main memory on  |
|                                   | your system, the `lscpu` command  |
|                                   | prints information about the CPU  |
|                                   | (including its cache capacities). |
|                                   | Running `free -m` shows the       |
|                                   | system's main memory capacity in  |
|                                   | megabytes.                        |
|                                   | :::                               |
+-----------------------------------+-----------------------------------+
:::
:::

::: sect2
### [](#_secondary_storage){.anchor}11.2.2. Secondary Storage {#_secondary_storage}

::: paragraph
Physically, secondary storage devices connect to a system even farther
away from the CPU than main memory. Compared to most other computer
equipment, secondary storage devices have evolved dramatically over the
years, and they continue to exhibit more diverse designs than other
components. The iconic [punch
card](https://en.wikipedia.org/wiki/Punched_card) allowed a human
operator to store data by making small holes in a thick piece of paper,
similar to an index card. Punch cards, whose design dates back to the
U.S. census of 1890, faithfully stored user data (often programs)
through the 1960s and into the 1970s.
:::

::: paragraph
A [tape drive](https://en.wikipedia.org/wiki/Magnetic_tape_data_storage)
stores data on a spool of magnetic tape. Although they generally offer
good storage density (lots of information in a small size) for a low
cost, tape drives are slow to access because they must wind the spool to
the correct location. Although most computer users don't encounter them
often anymore, tape drives are still frequently used for bulk storage
operations (for example, large data backups) in which reading the data
back is expected to be rare. Modern tape drives arrange the magnetic
tape spool into small cartridges for ease of use.
:::

::: {#FigStorageDevices .imageblock}
::: content
![Photos of classic secondary storage
devices.](_images/StorageDevices.png)
:::

::: title
Figure 2. Example photos of (a) a punch card, (b) a magnetic tape spool,
and (c) a variety of floppy disk sizes. Images from
[Wikipedia](https://www.wikipedia.org/).
:::
:::

::: paragraph
Removable media like [floppy
disks](https://en.wikipedia.org/wiki/Floppy_disk) and [optical
discs](https://en.wikipedia.org/wiki/Optical_disc) are another popular
form of secondary storage. Floppy disks contain a spindle of magnetic
recording media that rotates over a disk head that reads and writes its
contents. [Figure 2](#FigStorageDevices) shows photos of a punch card, a
tape drive, and a floppy disk. Optical discs such as CD, DVD, and
Blu-ray store information via small indentations on the disc. The drive
reads a disc by shining a laser at it, and the presence or absence of
indentations causes the beam to reflect (or not), encoding zeros and
ones.
:::

::: sect3
#### [](#_modern_secondary_storage){.anchor}Modern Secondary Storage {#_modern_secondary_storage}

+-----------------+-----------------+-----------------+-----------------+
| Device          | Capacity        | Latency         | Transfer rate   |
+=================+=================+=================+=================+
| Flash disk      | 0.5 - 2         | 0.1 - 1 ms      | 200 - 3,000     |
|                 | terabytes       |                 | megabytes /     |
|                 |                 |                 | second          |
+-----------------+-----------------+-----------------+-----------------+
| Traditional     | 0.5 - 10        | 5 - 10 ms       | 100 - 200       |
| hard disk       | terabytes       |                 | megabytes /     |
|                 |                 |                 | second          |
+-----------------+-----------------+-----------------+-----------------+
| Remote network  | Varies          | 20 - 200 ms     | Varies          |
| server          | considerably    |                 | considerably    |
+-----------------+-----------------+-----------------+-----------------+

: Table 2. Secondary Storage Device Characteristics of a Typical 2020
Workstation

::: paragraph
[Table 2](#TabSecondaryStorage) characterizes the secondary storage
devices commonly available to workstations today. [Figure 3](#FigIOBus)
displays how the path from secondary storage to main memory generally
passes through several intermediate device controllers. For example, a
typical hard disk connects to a Serial ATA controller, which connects to
the system I/O controller, which in turn connects to the memory bus.
These intermediate devices make disks easier to use by abstracting the
disk communication details from the OS and programmer. However, they
also introduce transfer delays as data flows through the additional
devices.
:::

::: {#FigIOBus .imageblock .text-center}
::: content
![The CPU cache is located on the CPU, in between the registers and the
CPU's connection to the memory bus. Also connected to the memory bus is
an I/O controller, which in turn connects to several other more specific
controllers like SATA, USB, and IDE.](_images/IOBus.png){width="800"}
:::

::: title
Figure 3. Secondary storage and I/O bus architecture
:::
:::

::: paragraph
The two most common secondary storage devices today are **hard disk
drives** (HDDs) and flash-based **solid-state drives** (SSDs). A hard
disk consists of a few flat, circular platters made from a material that
allows for magnetic recording. The platters rotate quickly, typically at
speeds between 5,000 and 15,000 revolutions per minute. As the platters
spin, a small mechanical arm with a disk head at the tip moves across
the platter to read or write data on concentric tracks (regions of the
platter located at the same diameter).
:::

::: paragraph
[Figure 4](#FigDiskParts) illustrates the major components of a [hard
disk](https://en.wikipedia.org/wiki/Hard_disk_drive). Before accessing
data, the disk must align the disk head with the track that contains the
desired data. Alignment requires extending or retracting the arm until
the head sits above the track. Moving the disk arm is called
**seeking**, and because it requires mechanical motion, seeking
introduces a small **seek time** delay to accessing data (a few
milliseconds). When the arm is in the correct position, the disk must
wait for the platter to rotate until the disk head is directly above the
location that stores the desired data. This introduces another short
delay (a few more milliseconds), known as **rotational latency**. Thus,
due to their mechanical characteristics, hard disks exhibit
significantly higher access latencies than the primary storage devices
described earlier.
:::

::: {#FigDiskParts .imageblock .text-center}
::: content
![A photo of the internals of a hard disk with its parts
labeled.](_images/DiskParts.png){width="650"}
:::

::: title
Figure 4. The major components of a hard disk drive
:::
:::

::: paragraph
In the past few years, SSDs, which have no moving parts (and thus lower
latency) have quickly risen to prominence. They are known as solid-state
drives because they don't rely on mechanical movement. Although several
solid-state technologies exist, [flash
memory](https://en.wikipedia.org/wiki/Flash_memory) reigns supreme in
commercial SSD devices. The technical details of flash memory are beyond
the scope of this book, but it suffices to say that flash-based devices
allow for reading, writing, and erasing data at speeds faster than
traditional hard disks. Though they don't yet store data as densely as
their mechanical counterparts, they've largely replaced spinning disks
in most consumer devices like laptops.
:::
:::
:::
:::
:::

::: toc-menu
:::
:::
:::
:::

Copyright (C) 2020 Dive into Systems, LLC.

*Dive into Systems,* is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0
International](https://creativecommons.org/licenses/by-nc-nd/4.0/) (CC
BY-NC-ND 4.0).
