<!DOCTYPE HTML>
<html lang="vi" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>To Exascale and Beyond - Đắm mình vào hệ thống - Dive into Systems</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../index.html"><strong aria-hidden="true">1.</strong> Home</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../copyright.html"><strong aria-hidden="true">1.1.</strong> Copyright</a></li><li class="chapter-item expanded "><a href="../acknowledgements.html"><strong aria-hidden="true">1.2.</strong> Acknowledgements</a></li><li class="chapter-item expanded "><a href="../preface.html"><strong aria-hidden="true">1.3.</strong> Preface</a></li></ol></li><li class="chapter-item expanded "><a href="../introduction.html"><strong aria-hidden="true">2.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="../C4-Binary/index.html"><strong aria-hidden="true">3.</strong> Binary and Data Representation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C4-Binary/bases.html"><strong aria-hidden="true">3.1.</strong> Number Bases and Unsigned Integers</a></li><li class="chapter-item expanded "><a href="../C4-Binary/conversion.html"><strong aria-hidden="true">3.2.</strong> Converting Between Bases</a></li><li class="chapter-item expanded "><a href="../C4-Binary/signed.html"><strong aria-hidden="true">3.3.</strong> Signed Binary Integers</a></li><li class="chapter-item expanded "><a href="../C4-Binary/arithmetic.html"><strong aria-hidden="true">3.4.</strong> Binary Integer Arithmetic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C4-Binary/arithmetic_addition.html"><strong aria-hidden="true">3.4.1.</strong> Addition</a></li><li class="chapter-item expanded "><a href="../C4-Binary/arithmetic_subtraction.html"><strong aria-hidden="true">3.4.2.</strong> Subtraction</a></li><li class="chapter-item expanded "><a href="../C4-Binary/arithmetic_mult_div.html"><strong aria-hidden="true">3.4.3.</strong> Multiplication & Division</a></li></ol></li><li class="chapter-item expanded "><a href="../C4-Binary/overflow.html"><strong aria-hidden="true">3.5.</strong> Overflow</a></li><li class="chapter-item expanded "><a href="../C4-Binary/bitwise.html"><strong aria-hidden="true">3.6.</strong> Bitwise Operators</a></li><li class="chapter-item expanded "><a href="../C4-Binary/byte_order.html"><strong aria-hidden="true">3.7.</strong> Integer Byte Order</a></li><li class="chapter-item expanded "><a href="../C4-Binary/floating_point.html"><strong aria-hidden="true">3.8.</strong> Real Numbers in Binary</a></li><li class="chapter-item expanded "><a href="../C4-Binary/summary.html"><strong aria-hidden="true">3.9.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../C5-Arch/index.html"><strong aria-hidden="true">4.</strong> What von Neumann Knew: Computer Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C5-Arch/hist.html"><strong aria-hidden="true">4.1.</strong> The Origins of Modern Computing</a></li><li class="chapter-item expanded "><a href="../C5-Arch/von.html"><strong aria-hidden="true">4.2.</strong> The von Neumann Architecture</a></li><li class="chapter-item expanded "><a href="../C5-Arch/gates.html"><strong aria-hidden="true">4.3.</strong> Logic Gates</a></li><li class="chapter-item expanded "><a href="../C5-Arch/circuits.html"><strong aria-hidden="true">4.4.</strong> Circuits</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C5-Arch/arithlogiccircs.html"><strong aria-hidden="true">4.4.1.</strong> Arithmetic and Logic Circuits</a></li><li class="chapter-item expanded "><a href="../C5-Arch/controlcircs.html"><strong aria-hidden="true">4.4.2.</strong> Control Circuits</a></li><li class="chapter-item expanded "><a href="../C5-Arch/storagecircs.html"><strong aria-hidden="true">4.4.3.</strong> Storage Circuits</a></li></ol></li><li class="chapter-item expanded "><a href="../C5-Arch/cpu.html"><strong aria-hidden="true">4.5.</strong> Building a Processor</a></li><li class="chapter-item expanded "><a href="../C5-Arch/instrexec.html"><strong aria-hidden="true">4.6.</strong> The Processor’s Execution of Program Instructions</a></li><li class="chapter-item expanded "><a href="../C5-Arch/pipelining.html"><strong aria-hidden="true">4.7.</strong> Pipelining Instruction Execution</a></li><li class="chapter-item expanded "><a href="../C5-Arch/pipelining_advanced.html"><strong aria-hidden="true">4.8.</strong> Advanced Pipelining Considerations</a></li><li class="chapter-item expanded "><a href="../C5-Arch/modern.html"><strong aria-hidden="true">4.9.</strong> Looking Ahead: CPUs Today</a></li><li class="chapter-item expanded "><a href="../C5-Arch/summary.html"><strong aria-hidden="true">4.10.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../C6-asm_intro/index.html"><strong aria-hidden="true">5.</strong> Under the C: Dive into Assembly</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/index.html"><strong aria-hidden="true">6.</strong> -bit x86 Assembly</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C7-x86_64/basics.html"><strong aria-hidden="true">6.1.</strong> Assembly Basics</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/common.html"><strong aria-hidden="true">6.2.</strong> Common Instructions</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/arithmetic.html"><strong aria-hidden="true">6.3.</strong> Additional Arithmetic Instructions</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/conditional_control_loops.html"><strong aria-hidden="true">6.4.</strong> Conditional Control and Loops</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C7-x86_64/preliminaries.html"><strong aria-hidden="true">6.4.1.</strong> Preliminaries</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/if_statements.html"><strong aria-hidden="true">6.4.2.</strong> If Statements</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/loops.html"><strong aria-hidden="true">6.4.3.</strong> Loops</a></li></ol></li><li class="chapter-item expanded "><a href="../C7-x86_64/functions.html"><strong aria-hidden="true">6.5.</strong> Functions in Assembly</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/recursion.html"><strong aria-hidden="true">6.6.</strong> Recursion</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/arrays.html"><strong aria-hidden="true">6.7.</strong> Arrays in Assembly</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/matrices.html"><strong aria-hidden="true">6.8.</strong> Matrices in Assembly</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/structs.html"><strong aria-hidden="true">6.9.</strong> Structs in Assembly</a></li><li class="chapter-item expanded "><a href="../C7-x86_64/buffer_overflow.html"><strong aria-hidden="true">6.10.</strong> Buffer Overflows</a></li></ol></li><li class="chapter-item expanded "><a href="../C8-IA32/index.html"><strong aria-hidden="true">7.</strong> 64-bit x86 Assembly</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C8-IA32/basics.html"><strong aria-hidden="true">7.1.</strong> Assembly Basics</a></li><li class="chapter-item expanded "><a href="../C8-IA32/common.html"><strong aria-hidden="true">7.2.</strong> Common Instructions</a></li><li class="chapter-item expanded "><a href="../C8-IA32/arithmetic.html"><strong aria-hidden="true">7.3.</strong> Additional Arithmetic Instructions</a></li><li class="chapter-item expanded "><a href="../C8-IA32/conditional_control_loops.html"><strong aria-hidden="true">7.4.</strong> Conditional Control and Loops</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C8-IA32/preliminaries.html"><strong aria-hidden="true">7.4.1.</strong> Preliminaries</a></li><li class="chapter-item expanded "><a href="../C8-IA32/if_statements.html"><strong aria-hidden="true">7.4.2.</strong> If Statements</a></li><li class="chapter-item expanded "><a href="../C8-IA32/loops.html"><strong aria-hidden="true">7.4.3.</strong> Loops</a></li></ol></li><li class="chapter-item expanded "><a href="../C8-IA32/functions.html"><strong aria-hidden="true">7.5.</strong> Functions in Assembly</a></li><li class="chapter-item expanded "><a href="../C8-IA32/recursion.html"><strong aria-hidden="true">7.6.</strong> Recursion</a></li><li class="chapter-item expanded "><a href="../C8-IA32/arrays.html"><strong aria-hidden="true">7.7.</strong> Arrays in Assembly</a></li><li class="chapter-item expanded "><a href="../C8-IA32/matrices.html"><strong aria-hidden="true">7.8.</strong> Matrices in Assembly</a></li><li class="chapter-item expanded "><a href="../C8-IA32/structs.html"><strong aria-hidden="true">7.9.</strong> Structs in Assembly</a></li><li class="chapter-item expanded "><a href="../C8-IA32/buffer_overflow.html"><strong aria-hidden="true">7.10.</strong> Buffer Overflows</a></li></ol></li><li class="chapter-item expanded "><a href="../C9-ARM64/index.html"><strong aria-hidden="true">8.</strong> ARMv8 Assembly</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C9-ARM64/basics.html"><strong aria-hidden="true">8.1.</strong> Assembly Basics</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/common.html"><strong aria-hidden="true">8.2.</strong> Common Instructions</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/arithmetic.html"><strong aria-hidden="true">8.3.</strong> Arithmetic Instructions</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/conditional_control_loops.html"><strong aria-hidden="true">8.4.</strong> Conditional Control and Loops</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C9-ARM64/preliminaries.html"><strong aria-hidden="true">8.4.1.</strong> Preliminaries</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/if_statements.html"><strong aria-hidden="true">8.4.2.</strong> If Statements</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/loops.html"><strong aria-hidden="true">8.4.3.</strong> Loops</a></li></ol></li><li class="chapter-item expanded "><a href="../C9-ARM64/functions.html"><strong aria-hidden="true">8.5.</strong> Functions in Assembly</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/recursion.html"><strong aria-hidden="true">8.6.</strong> Recursion</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/arrays.html"><strong aria-hidden="true">8.7.</strong> Arrays in Assembly</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/matrices.html"><strong aria-hidden="true">8.8.</strong> Matrices in Assembly</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/structs.html"><strong aria-hidden="true">8.9.</strong> Structs in Assembly</a></li><li class="chapter-item expanded "><a href="../C9-ARM64/buffer_overflow.html"><strong aria-hidden="true">8.10.</strong> Buffer Overflows</a></li></ol></li><li class="chapter-item expanded "><a href="../C10-asm_takeaways/index.html"><strong aria-hidden="true">9.</strong> Key Assembly Takeaways</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/index.html"><strong aria-hidden="true">10.</strong> Storage and the Memory Hierarchy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C11-MemHierarchy/mem_hierarchy.html"><strong aria-hidden="true">10.1.</strong> The Memory Hierarchy</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/devices.html"><strong aria-hidden="true">10.2.</strong> Storage Devices</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/locality.html"><strong aria-hidden="true">10.3.</strong> Locality</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/caching.html"><strong aria-hidden="true">10.4.</strong> Caching</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/cachegrind.html"><strong aria-hidden="true">10.5.</strong> Cache Analysis and Cachegrind</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/coherency.html"><strong aria-hidden="true">10.6.</strong> Looking Ahead: Caching on Multicore Processors</a></li><li class="chapter-item expanded "><a href="../C11-MemHierarchy/summary.html"><strong aria-hidden="true">10.7.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../C12-CodeOpt/index.html"><strong aria-hidden="true">11.</strong> Code Optimization</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C12-CodeOpt/basic.html"><strong aria-hidden="true">11.1.</strong> First Steps</a></li><li class="chapter-item expanded "><a href="../C12-CodeOpt/loops_functions.html"><strong aria-hidden="true">11.2.</strong> Other Compiler Optimizations</a></li><li class="chapter-item expanded "><a href="../C12-CodeOpt/memory_considerations.html"><strong aria-hidden="true">11.3.</strong> Memory Considerations</a></li><li class="chapter-item expanded "><a href="../C12-CodeOpt/summary.html"><strong aria-hidden="true">11.4.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../C13-OS/index.html"><strong aria-hidden="true">12.</strong> The Operating System</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C13-OS/impl.html"><strong aria-hidden="true">12.1.</strong> Booting and Running</a></li><li class="chapter-item expanded "><a href="../C13-OS/processes.html"><strong aria-hidden="true">12.2.</strong> Processes</a></li><li class="chapter-item expanded "><a href="../C13-OS/vm.html"><strong aria-hidden="true">12.3.</strong> Virtual Memory</a></li><li class="chapter-item expanded "><a href="../C13-OS/ipc.html"><strong aria-hidden="true">12.4.</strong> Interprocess Communication</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C13-OS/ipc_signals.html"><strong aria-hidden="true">12.4.1.</strong> Signals</a></li><li class="chapter-item expanded "><a href="../C13-OS/ipc_msging.html"><strong aria-hidden="true">12.4.2.</strong> Message Passing</a></li><li class="chapter-item expanded "><a href="../C13-OS/ipc_shm.html"><strong aria-hidden="true">12.4.3.</strong> Shared Memory</a></li></ol></li><li class="chapter-item expanded "><a href="../C13-OS/advanced.html"><strong aria-hidden="true">12.5.</strong> Summary and Other OS Functionality</a></li></ol></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/index.html"><strong aria-hidden="true">13.</strong> Leveraging Shared Memory in the Multicore Era</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C14-SharedMemory/multicore.html"><strong aria-hidden="true">13.1.</strong> Programming Multicore Systems</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/posix.html"><strong aria-hidden="true">13.2.</strong> POSIX Threads</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/synchronization.html"><strong aria-hidden="true">13.3.</strong> Synchronizing Threads</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C14-SharedMemory/mutex.html"><strong aria-hidden="true">13.3.1.</strong> Mutual Exclusion</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/semaphores.html"><strong aria-hidden="true">13.3.2.</strong> Semaphores</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/other_syncs.html"><strong aria-hidden="true">13.3.3.</strong> Other Synchronization Constructs</a></li></ol></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/performance.html"><strong aria-hidden="true">13.4.</strong> Measuring Parallel Performance</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C14-SharedMemory/performance_basics.html"><strong aria-hidden="true">13.4.1.</strong> Parallel Performance Basics</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/performance_advanced.html"><strong aria-hidden="true">13.4.2.</strong> Advanced Topics</a></li></ol></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/cache_coherence.html"><strong aria-hidden="true">13.5.</strong> Cache Coherence</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/thread_safety.html"><strong aria-hidden="true">13.6.</strong> Thread Safety</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/openmp.html"><strong aria-hidden="true">13.7.</strong> Implicit Threading with OpenMP</a></li><li class="chapter-item expanded "><a href="../C14-SharedMemory/summary.html"><strong aria-hidden="true">13.8.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../C15-Parallel/index.html"><strong aria-hidden="true">14.</strong> Looking Ahead: Other Parallel Systems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C15-Parallel/gpu.html"><strong aria-hidden="true">14.1.</strong> Hardware Acceleration and CUDA</a></li><li class="chapter-item expanded "><a href="../C15-Parallel/distrmem.html"><strong aria-hidden="true">14.2.</strong> Distributed Memory Systems</a></li><li class="chapter-item expanded "><a href="../C15-Parallel/cloud.html" class="active"><strong aria-hidden="true">14.3.</strong> To Exascale and Beyond</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Đắm mình vào hệ thống - Dive into Systems</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="153-tới-exascale-và-hơn-thế-nữa-Điện-toán-đám-mây-dữ-liệu-lớn-và-tương-lai-của-tính-toán"><a class="header" href="#153-tới-exascale-và-hơn-thế-nữa-Điện-toán-đám-mây-dữ-liệu-lớn-và-tương-lai-của-tính-toán">15.3. Tới Exascale và hơn thế nữa: Điện toán đám mây, Dữ liệu lớn và Tương lai của tính toán</a></h2>
<p>(<em>To Exascale and Beyond: Cloud Computing, Big Data, and the Future of Computing</em>)</p>
<p>Những tiến bộ trong công nghệ đã giúp con người có khả năng tạo ra dữ liệu với tốc độ chưa từng có.<br />
Các thiết bị khoa học như kính thiên văn, máy giải trình tự sinh học và cảm biến có thể tạo ra dữ liệu khoa học có độ chính xác cao với chi phí thấp.<br />
Khi các nhà khoa học phải vật lộn để phân tích “làn sóng dữ liệu” (<em>data deluge</em>) này, họ ngày càng dựa nhiều hơn vào các siêu máy tính đa node tinh vi, vốn là nền tảng của <strong>high-performance computing</strong> (HPC – tính toán hiệu năng cao).</p>
<p>Các ứng dụng HPC thường được viết bằng các ngôn ngữ như <strong>C</strong>, <strong>C++</strong> hoặc <strong>Fortran</strong>, với khả năng<br />
<a href="../C14-SharedMemory/multicore.html#_programming_multicore_systems">multithreading</a> và<br />
<a href="distrmem.html#_distributed_memory_systems_message_passing_and_mpi">message passing</a><br />
được hỗ trợ thông qua các thư viện như<br />
<a href="../C14-SharedMemory/posix.html#_hello_threading_writing_your_first_multithreaded_program">POSIX threads</a>,<br />
<a href="../C14-SharedMemory/openmp.html#_implicit_threading_with_openmp">OpenMP</a> và <strong>MPI</strong>.</p>
<p>Cho đến nay, phần lớn nội dung của cuốn sách này đã mô tả các đặc điểm kiến trúc, ngôn ngữ và thư viện thường được tận dụng trên các hệ thống HPC.<br />
Các công ty, phòng thí nghiệm quốc gia và các tổ chức quan tâm đến việc thúc đẩy khoa học thường sử dụng hệ thống HPC và tạo thành lõi của hệ sinh thái khoa học tính toán.</p>
<p>Trong khi đó, sự bùng nổ của các thiết bị kết nối internet và sự phổ biến của mạng xã hội đã khiến con người tạo ra một lượng lớn nội dung đa phương tiện trực tuyến một cách dễ dàng, dưới dạng trang web, hình ảnh, video, tweet và bài đăng mạng xã hội.<br />
Ước tính rằng <strong>90%</strong> tổng dữ liệu trực tuyến được tạo ra chỉ trong <strong>hai năm qua</strong>, và xã hội đang tạo ra <strong>30 terabyte dữ liệu người dùng mỗi giây</strong> (tương đương <strong>2,5 exabyte mỗi ngày</strong>).</p>
<p>Làn sóng <strong>dữ liệu người dùng</strong> này mang lại cho các công ty và tổ chức một kho thông tin khổng lồ về thói quen, sở thích và hành vi của người dùng, đồng thời hỗ trợ việc xây dựng hồ sơ khách hàng giàu dữ liệu để điều chỉnh sản phẩm và dịch vụ thương mại tốt hơn.</p>
<p>Để phân tích dữ liệu người dùng, các công ty thường dựa vào các <strong>trung tâm dữ liệu đa node</strong> (multinode data center) có nhiều thành phần kiến trúc phần cứng tương tự siêu máy tính.<br />
Tuy nhiên, các trung tâm dữ liệu này sử dụng một <strong>software stack</strong> khác, được thiết kế đặc biệt cho dữ liệu dựa trên internet.</p>
<p>Các hệ thống máy tính được sử dụng để lưu trữ và phân tích dữ liệu internet quy mô lớn đôi khi được gọi là <strong>high-end data analysis</strong> (HDA – phân tích dữ liệu cao cấp).<br />
Các công ty như Amazon, Google, Microsoft và Facebook có lợi ích trực tiếp trong việc phân tích dữ liệu internet và tạo thành lõi của hệ sinh thái phân tích dữ liệu.<br />
Cuộc cách mạng HDA và phân tích dữ liệu bắt đầu khoảng năm 2010 và hiện là một lĩnh vực chủ đạo trong nghiên cứu <strong>cloud computing</strong> (điện toán đám mây).</p>
<p>Hình 1 nêu bật những khác biệt chính trong phần mềm được sử dụng bởi cộng đồng HDA và HPC.<br />
Lưu ý rằng cả hai cộng đồng đều sử dụng phần cứng cụm (cluster hardware) tương tự, tuân theo mô hình<br />
<a href="distrmem.html#_distributed_memory_systems_message_passing_and_mpi">distributed memory</a>,<br />
trong đó mỗi <strong>compute node</strong> thường có một hoặc nhiều bộ xử lý<br />
<a href="../C14-SharedMemory/index.html#_leveraging_shared_memory_in_the_multicore_era">multicore</a><br />
và thường kèm theo GPU.</p>
<p>Phần cứng cụm thường bao gồm một <strong>distributed filesystem</strong> (hệ thống tệp phân tán) cho phép người dùng và ứng dụng truy cập chung vào các tệp nằm trên nhiều node trong cụm.</p>
<p><img src="_images/NewHPCHDAFigure.png" alt="High-end Data Analysis (HDA) vs High Performance Computing (HPC)." /></p>
<p><strong>Hình 1.</strong> So sánh các framework HDA và HPC. Dựa trên hình của Jack Dongarra và Daniel Reed.^8^</p>
<p>Không giống như siêu máy tính – vốn thường được xây dựng và tối ưu hóa cho mục đích HPC – cộng đồng HDA dựa vào <strong>data center</strong> (trung tâm dữ liệu), bao gồm một tập hợp lớn các compute node đa dụng, thường được kết nối mạng qua Ethernet.<br />
Ở cấp độ phần mềm, các trung tâm dữ liệu thường sử dụng <strong>máy ảo</strong> (virtual machine), <strong>cơ sở dữ liệu phân tán lớn</strong> và các <strong>framework</strong> cho phép phân tích dữ liệu internet với thông lượng cao.</p>
<p>Thuật ngữ <strong>cloud</strong> (đám mây) đề cập đến các thành phần lưu trữ dữ liệu và năng lực tính toán của các trung tâm dữ liệu HDA.</p>
<p>Trong phần này, chúng ta sẽ điểm qua <strong>cloud computing</strong>, một số phần mềm thường được sử dụng để triển khai cloud computing (đặc biệt là <strong>MapReduce</strong>), và một số thách thức trong tương lai.<br />
Lưu ý rằng phần này <strong>không</strong> nhằm mục đích đi sâu vào các khái niệm này; chúng tôi khuyến khích người đọc quan tâm tìm hiểu thêm từ các nguồn tham khảo được liệt kê.</p>
<h3 id="1531-cloud-computing"><a class="header" href="#1531-cloud-computing">15.3.1. Cloud Computing</a></h3>
<p><strong>Cloud computing</strong> (điện toán đám mây) là việc sử dụng hoặc thuê tài nguyên đám mây cho nhiều loại dịch vụ khác nhau.<br />
Cloud computing cho phép hạ tầng tính toán hoạt động như một <strong>dịch vụ tiện ích</strong> (<em>utility</em>): một số nhà cung cấp trung tâm cung cấp cho người dùng và tổ chức quyền truy cập (gần như vô hạn) vào sức mạnh tính toán thông qua internet, với việc người dùng và tổ chức lựa chọn sử dụng bao nhiêu tùy ý và trả tiền theo mức độ sử dụng.</p>
<p>Cloud computing có ba trụ cột chính:</p>
<ul>
<li><strong>Software as a Service (SaaS)</strong> – Phần mềm như một dịch vụ</li>
<li><strong>Infrastructure as a Service (IaaS)</strong> – Hạ tầng như một dịch vụ</li>
<li><strong>Platform as a Service (PaaS)</strong> – Nền tảng như một dịch vụ ^1^</li>
</ul>
<h4 id="software-as-a-service"><a class="header" href="#software-as-a-service">Software as a Service</a></h4>
<p><strong>Software as a Service</strong> (SaaS – Phần mềm như một dịch vụ) đề cập đến phần mềm được cung cấp trực tiếp cho người dùng thông qua <strong>cloud</strong> (đám mây).<br />
Hầu hết mọi người sử dụng trụ cột này của cloud computing mà thậm chí không nhận ra.<br />
Các ứng dụng mà nhiều người dùng hàng ngày (ví dụ: web mail, mạng xã hội, dịch vụ phát video trực tuyến) đều phụ thuộc vào hạ tầng cloud.</p>
<p>Hãy xét ví dụ kinh điển là dịch vụ web mail:<br />
Người dùng có thể đăng nhập và truy cập email của mình từ bất kỳ thiết bị nào, gửi và nhận thư, và dường như không bao giờ hết dung lượng lưu trữ.<br />
Các tổ chức quan tâm có thể “thuê” dịch vụ email trên cloud để cung cấp email cho khách hàng và nhân viên của mình, mà không phải chịu chi phí phần cứng và bảo trì khi tự vận hành dịch vụ.</p>
<p>Các dịch vụ thuộc trụ cột SaaS được quản lý hoàn toàn bởi nhà cung cấp cloud; tổ chức và người dùng (ngoài việc cấu hình một vài thiết lập nếu cần) không quản lý bất kỳ phần nào của ứng dụng, dữ liệu, phần mềm hay hạ tầng phần cứng — tất cả những thứ này sẽ cần thiết nếu họ tự triển khai dịch vụ trên phần cứng của mình.</p>
<p>Trước khi cloud computing ra đời, các tổ chức muốn cung cấp web mail cho người dùng sẽ cần có hạ tầng riêng và đội ngũ IT chuyên trách để duy trì.<br />
Ví dụ phổ biến về nhà cung cấp SaaS gồm <strong>Google’s G Suite</strong> và <strong>Microsoft Office 365</strong>.</p>
<h4 id="infrastructure-as-a-service"><a class="header" href="#infrastructure-as-a-service">Infrastructure as a Service</a></h4>
<p><strong>Infrastructure as a Service</strong> (IaaS – Hạ tầng như một dịch vụ) cho phép cá nhân và tổ chức “thuê” tài nguyên tính toán để đáp ứng nhu cầu của mình, thường dưới dạng truy cập <strong>máy ảo</strong> (virtual machine) — có thể là đa dụng hoặc được cấu hình sẵn cho một ứng dụng cụ thể.</p>
<p>Ví dụ kinh điển là dịch vụ <strong>Amazon Elastic Compute Cloud (EC2)</strong> của <strong>Amazon Web Services (AWS)</strong>.<br />
EC2 cho phép người dùng tạo máy ảo tùy chỉnh hoàn toàn.<br />
Thuật ngữ <strong>elastic</strong> (co giãn) trong EC2 đề cập đến khả năng của người dùng trong việc tăng hoặc giảm yêu cầu tài nguyên tính toán khi cần, và trả phí theo mức sử dụng.</p>
<p>Ví dụ: một tổ chức có thể sử dụng nhà cung cấp IaaS để lưu trữ website hoặc triển khai một loạt ứng dụng tùy chỉnh cho người dùng.<br />
Một số phòng thí nghiệm nghiên cứu và lớp học sử dụng dịch vụ IaaS thay cho máy trong phòng lab, chạy thí nghiệm trên cloud hoặc cung cấp nền tảng ảo cho sinh viên học tập.</p>
<p>Mục tiêu chung là loại bỏ chi phí bảo trì và đầu tư vốn để duy trì một cụm máy chủ hoặc server cá nhân cho các mục đích tương tự.<br />
Không giống SaaS, các trường hợp sử dụng IaaS yêu cầu khách hàng cấu hình ứng dụng, dữ liệu, và trong một số trường hợp là cả hệ điều hành của máy ảo.<br />
Tuy nhiên, hệ điều hành host và hạ tầng phần cứng được thiết lập và quản lý bởi nhà cung cấp cloud.</p>
<p>Các nhà cung cấp IaaS phổ biến gồm <strong>Amazon AWS</strong>, <strong>Google Cloud Services</strong> và <strong>Microsoft Azure</strong>.</p>
<h4 id="platform-as-a-service"><a class="header" href="#platform-as-a-service">Platform as a Service</a></h4>
<p><strong>Platform as a Service</strong> (PaaS – Nền tảng như một dịch vụ) cho phép cá nhân và tổ chức phát triển và triển khai ứng dụng web của riêng mình trên cloud, loại bỏ nhu cầu cấu hình hoặc bảo trì cục bộ.</p>
<p>Hầu hết các nhà cung cấp PaaS cho phép lập trình viên viết ứng dụng bằng nhiều ngôn ngữ khác nhau và cung cấp nhiều API để sử dụng.<br />
Ví dụ: dịch vụ của <strong>Microsoft Azure</strong> cho phép người dùng lập trình ứng dụng web trong <strong>Visual Studio IDE</strong> và triển khai ứng dụng lên Azure để kiểm thử.<br />
<strong>Google App Engine</strong> cho phép lập trình viên xây dựng và kiểm thử ứng dụng di động tùy chỉnh trên cloud bằng nhiều ngôn ngữ.<br />
<strong>Heroku</strong> và <strong>CloudBees</strong> là những ví dụ nổi bật khác.</p>
<p>Lưu ý rằng lập trình viên chỉ kiểm soát ứng dụng và dữ liệu của mình; nhà cung cấp cloud kiểm soát phần còn lại của hạ tầng phần mềm và toàn bộ hạ tầng phần cứng bên dưới.</p>
<h3 id="1532-mapreduce"><a class="header" href="#1532-mapreduce">15.3.2. MapReduce</a></h3>
<p>Có lẽ mô hình lập trình nổi tiếng nhất được sử dụng trên các hệ thống cloud là <strong>MapReduce</strong>^3^.<br />
Mặc dù nguồn gốc của MapReduce bắt nguồn từ các phép toán <strong>Map</strong> và <strong>Reduce</strong> trong lập trình hàm (functional programming), <strong>Google</strong> là công ty đầu tiên áp dụng khái niệm này để phân tích khối lượng lớn dữ liệu web.</p>
<p>MapReduce đã giúp Google thực hiện các truy vấn web nhanh hơn đối thủ, và góp phần đưa Google trở thành nhà cung cấp dịch vụ web và “gã khổng lồ” internet như ngày nay.</p>
<h4 id="hiểu-về-các-phép-toán-map-và-reduce"><a class="header" href="#hiểu-về-các-phép-toán-map-và-reduce">Hiểu về các phép toán Map và Reduce</a></h4>
<p>Các hàm <code>map</code> và <code>reduce</code> trong mô hình MapReduce dựa trên các phép toán <strong>Map</strong> và <strong>Reduce</strong> trong lập trình hàm.<br />
Trong phần này, chúng ta sẽ thảo luận ngắn gọn cách các phép toán này hoạt động, bằng cách xem lại một số ví dụ đã được trình bày trước đó trong sách.</p>
<p>Phép toán <strong>Map</strong> thường áp dụng cùng một hàm cho tất cả các phần tử trong một tập hợp.<br />
Những độc giả quen thuộc với Python có thể nhận ra chức năng này rõ nhất qua tính năng <strong>list comprehension</strong> (hiểu danh sách) trong Python.</p>
<p>Ví dụ, hai đoạn code dưới đây trong <a href="#ScalarMap">Bảng 1</a> thực hiện phép nhân vô hướng trong Python:</p>
<p>Chắc chắn rồi. Dưới đây là hai đoạn code đã được tách riêng.</p>
<h4 id="cách-nhân-vô-hướng-thông-thường"><a class="header" href="#cách-nhân-vô-hướng-thông-thường"><strong>Cách nhân vô hướng thông thường</strong></a></h4>
<pre><code class="language-python"># array là mảng số
# s là số nguyên
def scalarMultiply(array, s):
    for i in range(len(array)):
        array[i] = array[i] * s
    return array

# Gọi hàm
myArray = [1, 3, 5, 7, 9]
result = scalarMultiply(myArray, 2)
# In kết quả
print(result)
# [2, 6, 10, 14, 18]
</code></pre>
<h4 id="cách-nhân-vô-hướng-với-list-comprehension"><a class="header" href="#cách-nhân-vô-hướng-với-list-comprehension"><strong>Cách nhân vô hướng với list comprehension</strong></a></h4>
<pre><code class="language-python"># nhân hai số với nhau
def multiply(num1, num2):
    return num1 * num2

# array là mảng số
# s là số nguyên
def scalarMultiply(array, s):
    # dùng list comprehension
    return [multiply(x, s) for x in array]

# Gọi hàm
myArray = [1, 3, 5, 7, 9]
result = scalarMultiply(myArray, 2)
# In kết quả
print(result)
# [2, 6, 10, 14, 18]
</code></pre>
<p>Cú pháp <strong>list comprehension</strong> áp dụng cùng một hàm (trong trường hợp này là nhân một phần tử của mảng với giá trị vô hướng <code>s</code>) cho mọi phần tử <code>x</code> trong <code>array</code>.</p>
<p>Một phép toán <strong>Reduce</strong> đơn lẻ sẽ lấy một tập hợp các phần tử và kết hợp chúng lại thành một giá trị duy nhất bằng một hàm chung nào đó.<br />
Ví dụ: hàm <code>sum</code> trong Python hoạt động tương tự như một phép Reduce, vì nó nhận vào một tập hợp (thường là một danh sách Python) và cộng tất cả các phần tử lại với nhau.<br />
Chẳng hạn, áp dụng phép cộng cho tất cả các phần tử trong mảng <code>result</code> được trả về từ hàm <code>scalarMultiply</code> trong <strong>Bảng 1</strong> sẽ cho ra tổng cộng là <strong>50</strong>.</p>
<h4 id="mô-hình-lập-trình-mapreduce-the-mapreduce-programming-model"><a class="header" href="#mô-hình-lập-trình-mapreduce-the-mapreduce-programming-model">Mô hình lập trình MapReduce (The MapReduce Programming Model)</a></h4>
<p>Một đặc điểm quan trọng của MapReduce là <strong>mô hình lập trình đơn giản</strong>.<br />
Lập trình viên chỉ cần hiện thực hai loại hàm: <code>map</code> và <code>reduce</code>; phần còn lại sẽ được <strong>framework MapReduce</strong> tự động xử lý.</p>
<p>Hàm <code>map</code> do lập trình viên viết sẽ nhận vào một cặp (<em>key</em>, <em>value</em>) và xuất ra một loạt các cặp (<em>key</em>, <em>value</em>) trung gian, được ghi vào <strong>distributed filesystem</strong> (hệ thống tệp phân tán) dùng chung cho tất cả các node.<br />
Một <strong>combiner</strong> (thường được định nghĩa bởi framework MapReduce) sẽ gom nhóm các cặp (<em>key</em>, <em>value</em>) theo key, để tạo thành các cặp (<em>key</em>, list(<em>value</em>)) và chuyển chúng tới hàm <code>reduce</code> do lập trình viên định nghĩa.</p>
<p>Hàm <code>reduce</code> sẽ nhận vào một cặp (<em>key</em>, list(<em>value</em>)) và kết hợp tất cả các giá trị lại thông qua một phép toán do lập trình viên định nghĩa, để tạo ra cặp (<em>key</em>, <em>value</em>) cuối cùng, trong đó <em>value</em> là kết quả của phép giảm (reduction).<br />
Kết quả từ hàm <code>reduce</code> sẽ được ghi vào distributed filesystem và thường được trả về cho người dùng.</p>
<p>Để minh họa cách sử dụng mô hình MapReduce nhằm song song hóa một chương trình, chúng ta xét ví dụ <strong>Word Frequency</strong>.<br />
Mục tiêu của Word Frequency là xác định tần suất xuất hiện của mỗi từ trong một tập văn bản lớn.</p>
<p>Một lập trình viên C có thể hiện thực hàm <code>map</code> cho chương trình Word Frequency như sau:^3^</p>
<pre><code class="language-c">void map(char *key, char *value) {
    // key is document name
    // value is string containing some words (separated by spaces)
    int i;
    int numWords = 0; // number of words found: populated by parseWords()

    // returns an array of numWords words
    char *words[] = parseWords(value, &amp;numWords);
    for (i = 0; i &lt; numWords; i++) {
        // output (word, 1) key-value intermediate to file system
        emit(words[i], &quot;1&quot;);
    }
}
</code></pre>
<p>Hàm <code>map</code> này nhận vào một chuỗi (<code>key</code>) tương ứng với tên tệp, và một chuỗi khác (<code>value</code>) chứa một phần dữ liệu của tệp.<br />
Hàm sẽ tách các từ từ chuỗi <code>value</code> và phát ra (emit) từng từ (<code>words[i]</code>) kèm theo giá trị chuỗi <code>&quot;1&quot;</code>.<br />
Hàm <code>emit</code> được cung cấp bởi framework MapReduce và ghi các cặp (<em>key</em>, <em>value</em>) trung gian vào distributed filesystem.</p>
<p>Để hoàn thiện chương trình Word Frequency, lập trình viên có thể hiện thực hàm <code>reduce</code> như sau:</p>
<pre><code class="language-c">void reduce(char *key, struct Iterator values) {
    // key is individual word
    // value is of type Iterator (a struct that consists of
    // a items array (type char **), and its associated length (type int))
    int numWords = values.length();  // get length
    char *counts[] = values.items(); // get counts
    int i, total = 0;
    for (i = 0; i &lt; numWords; i++) {
        total += atoi(counts[i]); // sum up all counts
    }
    char *stringTotal = itoa(total); // convert total to a string
    emit(key, stringTotal); // output (word, total) pair to file system
}
</code></pre>
<p>Hàm <code>reduce</code> này nhận vào một chuỗi (<code>key</code>) tương ứng với một từ cụ thể, và một struct <code>Iterator</code> (cũng được cung cấp bởi framework MapReduce) bao gồm một mảng các phần tử đã được gom nhóm theo key (<code>items</code>) và độ dài của mảng đó (<code>length</code>).<br />
Trong ứng dụng Word Frequency, <code>items</code> là danh sách các số đếm.</p>
<p>Hàm sẽ lấy số lượng phần tử từ trường <code>length</code> của struct <code>Iterator</code>, và mảng số đếm từ trường <code>items</code>.<br />
Sau đó, nó lặp qua tất cả các số đếm, cộng dồn giá trị vào biến <code>total</code>.<br />
Vì hàm <code>emit</code> yêu cầu tham số kiểu <code>char *</code>, hàm sẽ chuyển đổi <code>total</code> sang chuỗi trước khi gọi <code>emit</code>.</p>
<p>Sau khi hiện thực <code>map</code> và <code>reduce</code>, trách nhiệm của lập trình viên kết thúc.<br />
Framework MapReduce sẽ tự động xử lý phần còn lại, bao gồm:</p>
<ul>
<li>Chia nhỏ dữ liệu đầu vào (partitioning the input)</li>
<li>Tạo và quản lý các tiến trình chạy hàm <code>map</code> (<strong>map tasks</strong>)</li>
<li>Gom nhóm và sắp xếp các cặp (<em>key</em>, <em>value</em>) trung gian</li>
<li>Tạo và quản lý các tiến trình chạy hàm <code>reduce</code> (<strong>reduce tasks</strong>)</li>
<li>Sinh ra tệp kết quả cuối cùng</li>
</ul>
<p>Để đơn giản, <strong>Hình 2</strong> minh họa cách MapReduce song song hóa các câu mở đầu của bài hát nổi tiếng <em>Code Monkey</em> của Jonathan Coulton:<br />
<em>code monkey get up get coffee, code monkey go to job</em>.</p>
<p><img src="_images/mapreduceEx.png" alt="Parallelization of the opening lines of the song Code Monkey using the MapReduce framework" /></p>
<p><strong>Hình 2.</strong> Song song hóa các câu mở đầu của bài hát <em>&quot;Code Monkey&quot;</em> bằng <strong>MapReduce framework</strong></p>
<p>Hình 2 cung cấp cái nhìn tổng quan về quá trình này.<br />
Trước khi thực thi, <strong>boss node</strong> (nút điều phối) sẽ chia dữ liệu đầu vào thành <em>M</em> phần, trong đó <em>M</em> tương ứng với số lượng <strong>map task</strong>.<br />
Trong <a href="#MapReduceFig">Hình 2</a>, <em>M</em> = 3, và tệp đầu vào (<code>coulton.txt</code>) được chia thành ba phần.</p>
<p>Trong <strong>map phase</strong> (giai đoạn map), boss node phân phối các map task cho một hoặc nhiều <strong>worker node</strong>, với mỗi map task được thực thi độc lập và song song.<br />
Ví dụ: map task đầu tiên phân tích đoạn <em>code monkey get up</em> thành các từ riêng biệt và phát ra (emit) bốn cặp (<em>key</em>, <em>value</em>):<br />
(<code>code</code>, <code>1</code>), (<code>monkey</code>, <code>1</code>), (<code>get</code>, <code>1</code>), (<code>up</code>, <code>1</code>).<br />
Mỗi map task sau đó ghi các giá trị trung gian này vào <strong>distributed filesystem</strong> (hệ thống tệp phân tán), chiếm một phần dung lượng lưu trữ trên mỗi node.</p>
<p>Trước khi bắt đầu <strong>reduce phase</strong> (giai đoạn reduce), framework sẽ gom nhóm và kết hợp các cặp (<em>key</em>, <em>value</em>) trung gian thành các cặp (<em>key</em>, list(<em>value</em>)).<br />
Trong Hình 2, ví dụ cặp (<code>get</code>, <code>1</code>) được phát ra bởi hai map task khác nhau.<br />
Framework MapReduce sẽ gom các cặp này thành một cặp duy nhất: (<code>get</code>, <code>[1,1]</code>).<br />
Các cặp trung gian đã được gom nhóm này sẽ được ghi xuống distributed filesystem trên đĩa.</p>
<p>Tiếp theo, framework MapReduce yêu cầu boss node tạo ra <em>R</em> <strong>reduce task</strong>.<br />
Trong Hình 2, <em>R</em> = 8.<br />
Framework sau đó phân phối các reduce task này cho các worker node.<br />
Một lần nữa, mỗi reduce task được thực thi độc lập và song song.</p>
<p>Trong giai đoạn reduce của ví dụ này, cặp (<code>get</code>, <code>[1,1]</code>) được giảm (reduce) thành cặp (<code>get</code>, <code>2</code>).<br />
Mỗi worker node sẽ nối kết quả của các reduce task mà nó xử lý vào tệp kết quả cuối cùng, tệp này sẽ sẵn sàng cho người dùng khi quá trình hoàn tất.</p>
<h4 id="fault-tolerance-khả-năng-chịu-lỗi"><a class="header" href="#fault-tolerance-khả-năng-chịu-lỗi">Fault Tolerance (Khả năng chịu lỗi)</a></h4>
<p>Các <strong>data center</strong> (trung tâm dữ liệu) thường chứa hàng nghìn node.<br />
Do đó, tỷ lệ hỏng hóc phần cứng là cao; ví dụ, nếu một node trong data center có 2% khả năng hỏng phần cứng, thì xác suất có ít nhất một node bị hỏng trong một data center 1.000 node là hơn 99,99%.</p>
<p>Phần mềm viết cho data center vì vậy phải <strong>fault tolerant</strong> (chịu lỗi), nghĩa là có thể tiếp tục hoạt động khi xảy ra sự cố phần cứng (hoặc ít nhất là dừng một cách an toàn).</p>
<p>MapReduce được thiết kế với khả năng chịu lỗi ngay từ đầu.<br />
Trong bất kỳ lần chạy MapReduce nào, sẽ có một boss node và có thể có hàng nghìn worker node.<br />
Khả năng một worker node bị hỏng là cao.</p>
<p>Để xử lý, boss node sẽ <strong>ping</strong> từng worker node theo chu kỳ.<br />
Nếu boss node không nhận được phản hồi từ một worker node, nó sẽ phân phối lại khối lượng công việc của worker đó sang một node khác và thực thi lại task^3^.</p>
<p>Nếu boss node bị hỏng (xác suất thấp vì chỉ có một node), job MapReduce sẽ bị hủy và phải chạy lại trên một node khác.<br />
Lưu ý rằng đôi khi worker node không phản hồi ping của boss node không phải vì hỏng, mà vì bị quá tải.<br />
MapReduce cũng áp dụng cùng chiến lược ping và phân phối lại công việc để giảm tác động của các worker node chậm (<strong>straggler node</strong>).</p>
<h4 id="hadoop-và-apache-spark"><a class="header" href="#hadoop-và-apache-spark">Hadoop và Apache Spark</a></h4>
<p>Sự ra đời của MapReduce đã tạo nên một làn sóng lớn trong giới công nghệ.<br />
Tuy nhiên, bản triển khai MapReduce của Google là <strong>closed source</strong> (mã nguồn đóng).<br />
Do đó, các kỹ sư tại Yahoo! đã phát triển <a href="https://hadoop.apache.org/"><strong>Hadoop</strong></a>, một bản triển khai <strong>open source</strong> (mã nguồn mở) của MapReduce, sau đó được <strong>Apache Foundation</strong> tiếp nhận.</p>
<p>Dự án Hadoop bao gồm một hệ sinh thái các công cụ cho Apache Hadoop, trong đó có <strong>Hadoop Distributed File System (HDFS)</strong> — một giải pháp mã nguồn mở thay thế <strong>Google File System</strong>, và <strong>HBase</strong> — được mô phỏng theo <strong>Google BigTable</strong>.</p>
<p>Hadoop có một số hạn chế chính:</p>
<ul>
<li>Thứ nhất, khó kết nối nhiều job MapReduce thành một <strong>workflow</strong> (quy trình) lớn hơn.</li>
<li>Thứ hai, việc ghi dữ liệu trung gian xuống HDFS trở thành nút thắt cổ chai, đặc biệt với các job nhỏ (dưới 1 GB).</li>
</ul>
<p><a href="https://spark.apache.org/"><strong>Apache Spark</strong></a> được thiết kế để giải quyết các vấn đề này và nhiều vấn đề khác.<br />
Nhờ các tối ưu hóa và khả năng xử lý phần lớn dữ liệu trung gian <strong>trong bộ nhớ</strong> (in-memory), Apache Spark có thể nhanh hơn Hadoop tới <strong>100 lần</strong> trên một số ứng dụng^4^.</p>
<h3 id="1533-hướng-tới-tương-lai-cơ-hội-và-thách-thức"><a class="header" href="#1533-hướng-tới-tương-lai-cơ-hội-và-thách-thức">15.3.3. Hướng tới tương lai: Cơ hội và Thách thức</a></h3>
<p>(<em>Looking Toward the Future: Opportunities and Challenges</em>)</p>
<p>Bất chấp những đổi mới trong cộng đồng phân tích dữ liệu internet, lượng dữ liệu mà nhân loại tạo ra vẫn tiếp tục tăng.<br />
Phần lớn dữ liệu mới được tạo ra trong cái gọi là <strong>edge environment</strong> (môi trường biên), tức là gần các cảm biến và các thiết bị tạo dữ liệu khác — vốn theo định nghĩa nằm ở phía “đầu kia” của mạng so với các nhà cung cấp <strong>commercial cloud</strong> (đám mây thương mại) và hệ thống <strong>HPC</strong> (high-performance computing – tính toán hiệu năng cao).</p>
<p>Truyền thống trước đây, các nhà khoa học và kỹ sư sẽ thu thập dữ liệu và phân tích nó bằng <strong>local cluster</strong> (cụm máy cục bộ), hoặc chuyển dữ liệu tới siêu máy tính hoặc trung tâm dữ liệu để phân tích.<br />
Cách tiếp cận “tập trung” này không còn khả thi nữa khi công nghệ cảm biến ngày càng phát triển, làm trầm trọng thêm “làn sóng dữ liệu” (<em>data deluge</em>).</p>
<p>Một nguyên nhân của sự tăng trưởng bùng nổ này là sự phổ biến của các thiết bị nhỏ có kết nối internet và chứa nhiều loại cảm biến.<br />
Những <strong>Internet of Things</strong> (IoT – Internet vạn vật) này đã tạo ra các tập dữ liệu lớn và đa dạng trong môi trường biên.<br />
Việc truyền các tập dữ liệu lớn từ edge lên cloud là khó khăn, vì dữ liệu càng lớn thì càng tốn nhiều thời gian và năng lượng để di chuyển.</p>
<p>Để giảm bớt các vấn đề hậu cần của cái gọi là <strong>Big Data</strong>, cộng đồng nghiên cứu đã bắt đầu tạo ra các kỹ thuật <strong>tóm tắt dữ liệu mạnh mẽ</strong> tại mỗi điểm truyền giữa edge và cloud^2^.<br />
Có sự quan tâm mạnh mẽ trong cộng đồng nghiên cứu máy tính về việc xây dựng hạ tầng có khả năng xử lý, lưu trữ và tóm tắt dữ liệu ngay tại môi trường biên trên một nền tảng thống nhất; lĩnh vực này được gọi là <strong>edge computing</strong> hoặc <strong>fog computing</strong>.</p>
<p>Edge computing đảo ngược mô hình phân tích Big Data truyền thống: thay vì phân tích diễn ra tại siêu máy tính hoặc trung tâm dữ liệu (“last mile”), thì phân tích diễn ra ngay tại nguồn tạo dữ liệu (“first mile”)^2^.</p>
<p>Ngoài vấn đề hậu cần di chuyển dữ liệu, một mối quan tâm xuyên suốt khác trong phân tích Big Data là <strong>quản lý năng lượng</strong>.<br />
Các tài nguyên tập trung lớn như siêu máy tính và trung tâm dữ liệu tiêu thụ rất nhiều năng lượng; các siêu máy tính hiện đại cần tới hàng megawatt (triệu watt) để vận hành và làm mát.</p>
<p>Một câu nói quen thuộc trong cộng đồng siêu máy tính là “<strong>a megawatt costs a megabuck</strong>” — nghĩa là cần khoảng 1 triệu USD mỗi năm để duy trì nhu cầu điện năng của 1 megawatt^5^.</p>
<p>Việc xử lý dữ liệu cục bộ trong môi trường biên giúp giảm bớt vấn đề di chuyển các tập dữ liệu lớn, nhưng hạ tầng tính toán tại đây cũng phải tiêu thụ năng lượng ở mức tối thiểu^2^.<br />
Đồng thời, việc tăng hiệu suất năng lượng của các siêu máy tính và trung tâm dữ liệu lớn là điều tối quan trọng^5^.</p>
<p>Ngoài ra, còn có sự quan tâm tới việc tìm cách <strong>hội tụ</strong> hai hệ sinh thái HPC và cloud computing để tạo ra một bộ framework, hạ tầng và công cụ chung cho phân tích dữ liệu quy mô lớn.<br />
Những năm gần đây, nhiều nhà khoa học đã sử dụng các kỹ thuật và công cụ do cộng đồng cloud computing phát triển để phân tích các tập dữ liệu HPC truyền thống, và ngược lại.</p>
<p>Việc hội tụ hai hệ sinh thái phần mềm này sẽ cho phép <strong>trao đổi chéo</strong> (cross-pollination) trong nghiên cứu và dẫn tới sự phát triển của một hệ thống thống nhất, cho phép cả hai cộng đồng cùng đối phó với làn sóng dữ liệu sắp tới, và thậm chí có thể chia sẻ tài nguyên.</p>
<p>Nhóm <a href="https://www.exascale.org/bdec/"><strong>Big Data Exascale Computing (BDEC)</strong></a> cho rằng thay vì coi HPC và cloud computing là hai mô hình hoàn toàn khác biệt, sẽ hữu ích hơn nếu xem cloud computing như một giai đoạn “<strong>digitally empowered</strong>” (được số hóa mạnh mẽ) của tính toán khoa học, trong đó nguồn dữ liệu ngày càng được tạo ra qua internet^2^.</p>
<p>Ngoài ra, cần có sự hội tụ về <strong>văn hóa, đào tạo và công cụ</strong> để tích hợp hoàn toàn cộng đồng phần mềm và nghiên cứu HPC với cloud computing.<br />
BDEC cũng đề xuất một mô hình trong đó các siêu máy tính và trung tâm dữ liệu là các “node” trong một mạng lưới tài nguyên tính toán rất lớn, cùng phối hợp để xử lý dữ liệu tràn về từ nhiều nguồn^2^.<br />
Mỗi node sẽ tóm tắt dữ liệu chảy vào nó một cách mạnh mẽ, chỉ chuyển tiếp lên node tài nguyên tính toán lớn hơn khi cần thiết.</p>
<p>Khi các hệ sinh thái cloud computing và HPC tìm kiếm sự thống nhất và chuẩn bị đối phó với làn sóng dữ liệu ngày càng tăng, tương lai của hệ thống máy tính hứa hẹn nhiều khả năng thú vị.<br />
Các lĩnh vực mới như <strong>trí tuệ nhân tạo</strong> (AI) và <strong>tính toán lượng tử</strong> đang dẫn tới sự ra đời của các <strong>domain-specific architecture</strong> (DSA – kiến trúc chuyên biệt theo lĩnh vực) và <strong>application-specific integrated circuit</strong> (ASIC – mạch tích hợp chuyên dụng) mới, có khả năng xử lý các quy trình công việc tùy chỉnh hiệu quả năng lượng hơn trước (xem ví dụ về <strong>TPU</strong>^6^).</p>
<p>Bên cạnh đó, vấn đề <strong>bảo mật</strong> của các kiến trúc này — vốn lâu nay bị cộng đồng bỏ qua — sẽ trở nên quan trọng khi dữ liệu mà chúng phân tích ngày càng có giá trị.<br />
Các kiến trúc mới cũng sẽ dẫn tới sự xuất hiện của các ngôn ngữ lập trình mới để lập trình cho chúng, và có thể cả các hệ điều hành mới để quản lý các giao diện khác nhau.</p>
<p>Để tìm hiểu thêm về tương lai của kiến trúc máy tính, chúng tôi khuyến khích bạn đọc bài viết <a href="https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext">này</a> của hai huyền thoại kiến trúc máy tính, đồng thời là chủ nhân Giải thưởng Turing ACM 2017 — <strong>John Hennessy</strong> và <strong>David Patterson</strong>^7^.</p>
<h3 id="tài-liệu-tham-khảo"><a class="header" href="#tài-liệu-tham-khảo">Tài liệu tham khảo</a></h3>
<ol>
<li>Armbrust et. al. <em>A view of cloud computing</em>. CACM 53(4). 2010.</li>
<li>Asch et. al. <em>Big data and extreme-scale computing: Pathways to Convergence – Toward a shaping strategy for a future software and data ecosystem for scientific inquiry</em>. The International Journal of High Performance Computing Applications 32(4), 435–479. 2018.</li>
<li>Dean and Ghemawat. <em>MapReduce: Simplified Data Processing on Large Clusters</em>. USENIX. 2004.</li>
<li>DataBricks. <em>Apache Spark</em>. <a href="https://databricks.com/spark/about">https://databricks.com/spark/about</a></li>
<li>M. Halper. <em>Supercomputing's Super Energy Needs, and What to Do About Them</em>. CACM News: <a href="https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext">https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext</a></li>
<li>Jouppi et. al. <em>In-datacenter performance analysis of a tensor processing unit</em>. 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), pp. 1–12. 2017.</li>
<li>J. Hennessy and D. Patterson. <em>A New Golden Age for Computer Architecture</em>. CACM 62(2), 48–60. 2019.</li>
<li>D. A. Reed and J. Dongarra. <em>Exascale computing and big data</em>. CACM 58(7), 56–68. 2015.</li>
</ol>
<p>Bạn có muốn tôi tiếp tục dịch sang <strong>Chương 16 – Operating Systems</strong> để nối tiếp nội dung cuốn sách không?</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../C15-Parallel/distrmem.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../C15-Parallel/distrmem.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
    </body>
</html>
